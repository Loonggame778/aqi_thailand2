{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "#  always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import src\n",
    "from src.imports import *\n",
    "from src.gen_functions import *\n",
    "from src.features.map_dataset import MapDataset\n",
    "# import the Dataset object class\n",
    "from src.features.dataset import Dataset\n",
    "from src.features.landuse import *\n",
    "from src.visualization.mapper import *\n",
    "from src.visualization.vis_data import *\n",
    "\n",
    "from src.data.fire_data import process_fire_data, add_datetime_fire, add_merc_to_fire\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import io\n",
    "import geopandas as gpd\n",
    "from fiona.io import ZipMemoryFile\n",
    "import fiona\n",
    "import shutil\n",
    "import re\n",
    "import pyproj\n",
    "\n",
    "# for merging landuse data \n",
    "from geopandas.tools import sjoin\n",
    "from shapely.geometry import Point\n",
    "from joblib import Parallel\n",
    "from textwrap import wrap\n",
    "from collections import defaultdict\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "#set bokeh output\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_folder = '../data/world_maps/'\n",
    "mfire_folder = '../data/fire_map/world_2000-2020/M6_proc/'\n",
    "vfire_folder = '../data/fire_map/world_2000-2020/V1_proc/'\n",
    "poll_folder = '../data/poll_map/'\n",
    "report_folder = '../reports/map/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire Thailand 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load country boundarys and abbreviation\n",
    "country_gdf = get_country_gdf()\n",
    "country_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = map_folder + 'THA.gdb'\n",
    "# select province level\n",
    "prov_map = gpd.read_file(filename, driver='FileGDB', layer=2)\n",
    "prov_map['geometry'].shape\n",
    "# overide old crs and convert\n",
    "crs = pyproj.CRS('EPSG:4326')\n",
    "prov_map['geometry'] = prov_map['geometry'].set_crs(crs, allow_override=True)\n",
    "prov_map['geometry'] = prov_map['geometry'].to_crs('EPSG:6933')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_list = []\n",
    "\n",
    "for s in prov_map['geometry'].to_crs('EPSG:3395'):\n",
    "    try:\n",
    "        temp = np.vstack(s.boundary.coords.xy).transpose()[:-1, :]\n",
    "        coord_list.append(temp )\n",
    "\n",
    "    except:\n",
    "        for b in s.boundary:\n",
    "            temp = np.vstack(b.coords.xy).transpose()[:-1, :]\n",
    "            coord_list.append(temp)\n",
    "\n",
    "coord_arr = np.vstack(coord_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_arr = coord_arr.flatten()\n",
    "coord_arr = coord_arr.reshape((-1, 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a center coordinate\n",
    "dataset = Dataset('Bangkok')\n",
    "x = dataset.city_info['long_m']\n",
    "y = dataset.city_info['lat_m']\n",
    "#stepx = 2E5\n",
    "\n",
    "stepx = 0.7E6\n",
    "stepy = stepx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range bounds supplied in web mercator coordinates\n",
    "p = figure(x_range=(x-1*stepx,x+1*stepx), y_range=(y-stepy, y+stepy ),\n",
    "           x_axis_type=\"mercator\", y_axis_type=\"mercator\", plot_width=1000, plot_height=1000)\n",
    "\n",
    "p.add_tile(get_provider(Vendors.STAMEN_TERRAIN_RETINA))\n",
    "p.line(coord_arr[:, 0], coord_arr[:, 1], color='black', legend_label='province border')\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(mfire_folder + '*.csv')\n",
    "th_fire_19 = []\n",
    "start_date = '2019-09-01'\n",
    "end_date = '2020-05-15'\n",
    "\n",
    "for file in tqdm(files):\n",
    "    fire = pd.read_csv(file)\n",
    "    # if 'country' not in fire:\n",
    "    # fire = add_countries(fire=fire, filename=file)\n",
    "    if (fire['acq_date'].str.contains('2019') | fire['acq_date'].str.contains('2020')).sum():\n",
    "        fire = add_datetime_fire(fire=fire, timezone=dataset.city_info['Time Zone']).set_index('datetime')\n",
    "        # select only Thailand fires and during 2019 pollution season\n",
    "        fire = fire.loc[start_date:end_date]\n",
    "    else:\n",
    "        fire = pd.DataFrame()\n",
    "    \n",
    "    if len(fire) > 0:\n",
    "        if 'country' not in fire:\n",
    "            fire = add_countries(df=fire)\n",
    "        fire = fire[fire['country'] == 'Thailand']\n",
    "        th_fire_19.append(fire)\n",
    "    \n",
    "th_fire_19 = pd.concat(th_fire_19, ignore_index=True)\n",
    "th_fire_19['distance'] = 0\n",
    "print(th_fire_19.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_fire_19 = add_datetime_fire(fire=th_fire_2019, timezone=dataset.city_info['Time Zone']).set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_fire_19.to_csv(poll_folder + 'th_fire_2019.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_province(p, gdf, col='admin1Name_en'):\n",
    "    \"\"\"Find a province hosting the hotspot.\n",
    "\n",
    "    Args:\n",
    "        p: Point object\n",
    "        gdf: geopandas dataframe with albel \n",
    "    \n",
    "    Returns: str \n",
    "        name of the country \n",
    "    \"\"\"\n",
    "    try: \n",
    "        province = gdf[gdf['geometry'].contains(p)][col].values[0]\n",
    "    except: \n",
    "        province = np.nan\n",
    "        \n",
    "    return province"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# add fire landuse \n",
    "label_landuse_fire(poll_folder + 'th_fire_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read fires in 2019 seasons\n",
    "th_fire_19 = pd.read_csv(poll_folder + 'th_fire_2019_label.csv')\n",
    "th_fire_19['datetime'] = pd.to_datetime(th_fire_19['datetime'])\n",
    "th_fire_19 = th_fire_19.set_index('datetime')\n",
    "th_fire_19['count'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_fire_19.resample('d').count()['count'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = map_folder + 'THA.gdb'\n",
    "# select province level\n",
    "prov_map = gpd.read_file(filename, driver='FileGDB', layer=2)\n",
    "prov_map['geometry'].shape\n",
    "# overide old crs and convert\n",
    "crs = pyproj.CRS('EPSG:4326')\n",
    "prov_map['geometry'] = prov_map['geometry'].set_crs(crs, allow_override=True)\n",
    "#prov_map['geometry'] = prov_map['geometry'].to_crs('EPSG:6933')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add province \n",
    "th_fire_19['geometry'] = [Point(x,y) for x, y in zip(th_fire_19['longitude'], th_fire_19['latitude'])]\n",
    "th_fire_19['province'] = th_fire_19['geometry'].swifter.apply(locate_province, gdf=prov_map, col='admin1Name_en')\n",
    "th_fire_19 = th_fire_19.drop('geometry', axis=1)\n",
    "th_fire_19.to_csv(poll_folder + 'th_fire_2019_label.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read fires in 2019 seasons\n",
    "th_fire_19 = pd.read_csv(poll_folder + 'th_fire_2019_label.csv')\n",
    "th_fire_19['datetime'] = pd.to_datetime(th_fire_19['datetime'])\n",
    "th_fire_19 = th_fire_19.set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_fire_19.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provinces with most burning \n",
    "count_prov = th_fire_19.groupby('province').count()[['count']]\n",
    "count_prov = count_prov.sort_values('count', ascending=False)\n",
    "\n",
    "spot_map = prov_map.merge(count_prov.reset_index(), left_on='admin1Name_en', right_on='province', how='left')\n",
    "spot_map['count'] = spot_map['count'].fillna(0)\n",
    "spot_map['density'] = spot_map['count']/spot_map['Shape_Area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "count_prov = spot_map.sort_values('count', ascending=False)[['province', 'count']].set_index('province')\n",
    "count_prov.head(30).plot(kind='bar', ax=ax)\n",
    "ax.set_title('Provice with most Hotspots (2019 season)')\n",
    "ax.set_ylabel('count')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(report_folder + 'th_hotspots_province_bar.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_prov = spot_map.sort_values('density', ascending=False)[['province', 'density']].set_index('province')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "count_prov.head(30).plot(kind='bar', ax=ax)\n",
    "ax.set_title('Provice with most Hotspots Density(2019 season)')\n",
    "ax.set_ylabel('density')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(10,10))\n",
    "spot_map.plot(column='count',\n",
    "           ax=ax[0],legend=True,\n",
    "           legend_kwds={'label': \"number of hotspots\",\n",
    "                       'orientation': \"horizontal\"},  cmap='OrRd')\n",
    "\n",
    "spot_map.plot(column='density',\n",
    "           ax=ax[1],legend=True,\n",
    "           legend_kwds={'label': \"density\",\n",
    "                       'orientation': \"horizontal\"},  cmap='OrRd')\n",
    "\n",
    "spot_map.boundary.plot(ax=ax[0], color='black')\n",
    "spot_map.boundary.plot(ax=ax[1], color='black')\n",
    "\n",
    "\n",
    "ax[0].set_title(\"number of hotspots\")\n",
    "ax[1].set_title(\"density of hotspots\")\n",
    "\n",
    "plt.savefig(report_folder + 'th_hotspots_province.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['forest', 'crop', 'shrubland', 'urban']\n",
    "fig, ax = plt.subplots(1, len(labels), figsize=(15, 8) )\n",
    "\n",
    "for label, a in zip(labels, ax):\n",
    "    temp = th_fire_19[th_fire_19['label']==label]\n",
    "    temp = temp.groupby('province').count()[['count']]\n",
    "    \n",
    "    \n",
    "    temp = prov_map.merge(temp.reset_index(), left_on='admin1Name_en', right_on='province', how='left')\n",
    "    temp['count'] = temp['count'].fillna(0)\n",
    "     \n",
    "   \n",
    "    temp.plot(column='count',\n",
    "           ax=a,legend=True,\n",
    "           legend_kwds={'orientation': \"horizontal\"},  cmap='OrRd')\n",
    "    temp.boundary.plot(ax=a, color='black')\n",
    "\n",
    "    a.set_title(f\"{label}\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(report_folder + 'th_hotspots_province_land.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['forest', 'crop']\n",
    "fig, ax = plt.subplots(len(labels), 1, figsize=(10, 10))\n",
    "\n",
    "for label, a in zip(labels, ax):\n",
    "    temp = th_fire_19[th_fire_19['label']==label]\n",
    "    temp = temp.groupby('province').count()[['count']]\n",
    "    \n",
    "    temp = temp.sort_values('count', ascending=False)\n",
    "    temp.head(30).plot(kind='bar', ax=a)\n",
    "    a.legend([label])\n",
    "    \n",
    "ax[0].set_title('Top Burning in forest and crop lands')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(report_folder + 'th_hotspots_province_land_bar.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire Thailand Build Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = map_folder + 'THA.gdb'\n",
    "# select province level\n",
    "prov_map = gpd.read_file(filename, driver='FileGDB', layer=2)\n",
    "prov_map['geometry'].shape\n",
    "# overide old crs and convert\n",
    "crs = pyproj.CRS('EPSG:4326')\n",
    "prov_map['geometry'] = prov_map['geometry'].set_crs(crs, allow_override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check columns \n",
    "files = glob(mfire_folder + '*.csv')\n",
    "\n",
    "columns_list = []\n",
    "for file in files:\n",
    "    fire = pd.read_csv(file)\n",
    "    columns_list += fire.columns.to_list()\n",
    "    \n",
    "columns_list = list(set(columns_list))\n",
    "print(columns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('Bangkok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(vfire_folder + '*.csv')\n",
    "#start_date = '2019-09-01'\n",
    "#end_date = '2020-05-15'\n",
    "\n",
    "filename = poll_folder + 'tha_fire_v.csv'\n",
    "drop_col = [ 'track', 'scan', 'frp', 'satellite', 'instrument','version']\n",
    "country = 'Thailand'\n",
    "\n",
    "fire_all = []\n",
    "\n",
    "for file in tqdm(files):\n",
    "    fire = pd.read_csv(file)\n",
    "    \n",
    "    if 'country' not in fire.columns:\n",
    "        print(file)\n",
    "        fire = add_countries(df=fire, filename=file)\n",
    "    \n",
    "    fire = fire[fire['country'] == country]\n",
    "    # add province \n",
    "    fire['geometry'] = [Point(x,y) for x, y in zip(fire['longitude'], fire['latitude'])]\n",
    "    fire['province'] = fire['geometry'].swifter.apply(locate_province, gdf=prov_map, col='admin1Name_en')\n",
    "    fire = fire.drop('geometry', axis=1)\n",
    "    \n",
    "    fire = add_datetime_fire(fire=fire, timezone=dataset.city_info['Time Zone']) \n",
    "    fire = fire.drop(drop_col, axis=1)\n",
    "    try:\n",
    "        fire = fire.drop('type', axis=1)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    fire_all.append(fire)\n",
    "    #temp = pd.DataFrame(columns= ['latitude',  'acq_date', 'acq_time', 'bright_t31', 'lat_km', 'daynight', 'longitude', 'confidence', 'version', 'country', 'long_km'])\n",
    "    #fire = pd.concat([temp, fire], ignore_index=True)\n",
    "fire_all = pd.concat(fire_all)\n",
    "fire_all = fire_all.drop_duplicates()\n",
    "fire_all.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = poll_folder + 'tha_fire_v.csv'\n",
    "\n",
    "# add landuse \n",
    "label_landuse_fire(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge country, province label with the landuse label\n",
    "filename = poll_folder + 'tha_fire_v.csv'\n",
    "\n",
    "th_fire = pd.read_csv(filename)\n",
    "th_fire['datetime'] = pd.to_datetime(th_fire['datetime'] )\n",
    "\n",
    "filename = poll_folder + 'tha_fire_v_label.csv'\n",
    "\n",
    "th_fire_label = pd.read_csv(filename)\n",
    "th_fire_label['datetime'] = pd.to_datetime(th_fire_label['datetime'] )\n",
    "\n",
    "th_fire_label = th_fire.merge(th_fire_label, on =['latitude','longitude', 'datetime'])\n",
    "\n",
    "th_fire_label.to_csv(poll_folder + 'tha_fire_v_label.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_fire_label.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire Thailand all year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = poll_folder + 'tha_fire_label.csv'\n",
    "\n",
    "th_fire_label = pd.read_csv(filename)\n",
    "th_fire_label['datetime'] = pd.to_datetime(th_fire_label['datetime'] )\n",
    "th_fire_label['year'] = th_fire_label['datetime'].dt.year\n",
    "th_fire_label['count'] = 1\n",
    "th_fire_label = add_season(th_fire_label.set_index('datetime'),  start_month='-09-01', end_month='-05-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_fire_label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trend over the year breakdown by landuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = th_fire_label.groupby(['year', 'label'], as_index=False).count() \n",
    "# drop year 2020\n",
    "df = df[df['year']< 2020]\n",
    "df = df[df['year']> 2002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,1,figsize=(10,4))\n",
    "sns.lineplot(x='year', y='count', hue='label', data=df, marker='o')\n",
    "\n",
    "land_list =['crop', 'forest', 'shrubland']\n",
    "colors = ['royalblue','orange', 'green']\n",
    "for land, color in zip(land_list, colors):\n",
    "    temp = df[df['label'] == land].set_index('year')['count']\n",
    "    temp = temp.loc['2009':]\n",
    "    add_ln_trend_line(temp, ax, color=color)\n",
    "    temp = temp.loc['2014':]\n",
    "    add_ln_trend_line(temp, ax, color=color)\n",
    "\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax.legend(land_list, bbox_to_anchor=(1.1, 1.05), frameon=True)\n",
    "ax.set_xlabel('season year')\n",
    "ax.set_ylabel('hotspots count')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I consider shrubland as forest, we will have the crop burning as high as the forest, but I think shrubland is a mix for forest and crop. Seeing that they crop and forest correlated, I still want to separated shrubland as another category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_fire_label_combine = th_fire_label.copy()\n",
    "th_fire_label_combine['label'] = th_fire_label_combine['label'].str.replace('shrubland', 'forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis with shrubland the same as forrest\n",
    "df = th_fire_label_combine.groupby(['year', 'label'], as_index=False).count() \n",
    "df = df[df['year']< 2020]\n",
    "_, ax = plt.subplots(1,1,figsize=(10,4))\n",
    "sns.lineplot(x='year', y='count', hue='label', data=df, marker='o')\n",
    "\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax.legend(bbox_to_anchor=(1.02, 1.05), frameon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the statitically significant of the trend \n",
    "\n",
    "land_list =['crop', 'forest', 'shrubland']\n",
    "for land in land_list:\n",
    "    temp = df[df['label'] == land].set_index('year')['count']\n",
    "    temp = temp.loc['2009':]\n",
    "    print(land, 'spearman', spearmanr(temp.index.values, temp.values))\n",
    "    print(land, 'pearson', pearsonr(temp.index.values, temp.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the statitically significant of the trend \n",
    "\n",
    "land_list =['crop', 'forest', 'shrubland']\n",
    "for land in land_list:\n",
    "    temp = df[df['label'] == land].set_index('year')['count']\n",
    "    temp = temp.loc['2014':]\n",
    "    print(land, 'spearman', spearmanr(temp.index.values, temp.values))\n",
    "    print(land, 'pearson', pearsonr(temp.index.values, temp.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breakdown by province, but not landuse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = th_fire_label.groupby(['year', 'province'], as_index=False).count() \n",
    "df = df[df['year']< 2020]\n",
    "land_list = ['crop', 'forest', 'shrubland']\n",
    "prov_list = df['province'].unique()\n",
    "\n",
    "trend_prov = []\n",
    "for prov in prov_list:\n",
    "    temp = df[ (df['province'] == prov)].set_index('year')['count']\n",
    "    temp = temp.loc['2009':]\n",
    "    coeff, p = spearmanr(temp.index.values, temp.values)\n",
    "    trend_dict =  {'province':prov, \n",
    "                  'coeff': coeff,\n",
    "                  'p-value':p}\n",
    "    trend_prov.append(    trend_dict )\n",
    "    \n",
    "trend_prov = pd.DataFrame(trend_prov)\n",
    "trend_prov = trend_prov.dropna()\n",
    "trend_prov = trend_prov.sort_values('p-value')\n",
    "\n",
    "# province with statistically significant trend\n",
    "\n",
    "trend_10y = trend_prov[trend_prov['p-value'] <= 0.05]\n",
    "trend_10y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = th_fire_label.groupby(['year', 'province'], as_index=False).count() \n",
    "df = df[df['year']< 2020]\n",
    "land_list = ['crop', 'forest', 'shrubland']\n",
    "prov_list = df['province'].unique()\n",
    "\n",
    "trend_prov = []\n",
    "for prov in prov_list:\n",
    "    temp = df[ (df['province'] == prov)].set_index('year')['count']\n",
    "    temp = temp.loc['2014':]\n",
    "    coeff, p = spearmanr(temp.index.values, temp.values)\n",
    "    trend_dict =  {'province':prov, \n",
    "                  'coeff': coeff,\n",
    "                  'p-value':p}\n",
    "    trend_prov.append(    trend_dict )\n",
    "    \n",
    "trend_prov = pd.DataFrame(trend_prov)\n",
    "trend_prov = trend_prov.dropna()\n",
    "trend_prov = trend_prov.sort_values('p-value')\n",
    "\n",
    "# province with statistically significant trend\n",
    "\n",
    "trend_5y = trend_prov[trend_prov['p-value'] <= 0.05]\n",
    "trend_5y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select category to inspect \n",
    "to_inspect_incr =  trend_10y[trend_10y['coeff'] > 0].reset_index(drop=True) \n",
    "to_inspect_decr =  trend_10y[trend_10y['coeff'] < 0].reset_index(drop=True)\n",
    "\n",
    "to_inspect_incr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_inspect_decr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(3,1,figsize=(10,12))\n",
    "\n",
    "\n",
    "trend_prov = []\n",
    "for i, row in to_inspect_incr.iterrows():\n",
    "    temp = df[(df['province'] == row['province'])].set_index('year')['count']\n",
    "    temp = temp.loc['2009':]\n",
    "    \n",
    "    sns.regplot( x=temp.index, y=temp.values, ax=ax[0], label=row['province'] )\n",
    "    \n",
    "for i, row in to_inspect_decr.iterrows():\n",
    "    temp = df[(df['province'] == row['province'])].set_index('year')['count']\n",
    "    temp = temp.loc['2009':]  \n",
    "    \n",
    "    if row['province'] == 'Nan':\n",
    "        \n",
    "        sns.regplot( x=temp.index, y=temp.values, ax=ax[1], label=row['province'] )\n",
    "        \n",
    "    else:\n",
    "        sns.regplot( x=temp.index, y=temp.values, ax=ax[2], label=row['province'] )\n",
    "         \n",
    "for a in ax:\n",
    "    a.legend(bbox_to_anchor=(1.02, 1.05), frameon=True)\n",
    "    \n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Province and Landuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trend breakdown by landuse \n",
    "df = th_fire_label.groupby(['year', 'province', 'label'], as_index=False).count() \n",
    "df = df[df['year']< 2020]\n",
    "land_list = ['crop', 'forest', 'shrubland']\n",
    "prov_list = df['province'].unique()\n",
    "\n",
    "trend_prov = []\n",
    "for prov, land in product(prov_list, land_list):\n",
    "    temp = df[(df['label'] == land) & (df['province'] == prov)].set_index('year')['count']\n",
    "    temp = temp.loc['2009':]\n",
    "    coeff, p = spearmanr(temp.index.values, temp.values)\n",
    "    trend_dict =  {'province':prov, \n",
    "                  'label':land,\n",
    "                  'coeff': coeff,\n",
    "                  'p-value':p}\n",
    "    trend_prov.append(    trend_dict )\n",
    "    \n",
    "trend_prov = pd.DataFrame(trend_prov)\n",
    "trend_prov = trend_prov.dropna()\n",
    "trend_prov = trend_prov.sort_values('p-value')\n",
    "\n",
    "# province with statistically significant trend\n",
    "\n",
    "trend_10y_land = trend_prov[trend_prov['p-value'] <= 0.02]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select category to inspect \n",
    "to_inspect_land_incr =  trend_10y_land[trend_10y_land['coeff'] > 0].reset_index(drop=True) \n",
    "to_inspect_land_decr =  trend_10y_land[trend_10y_land['coeff'] < 0].reset_index(drop=True)\n",
    "\n",
    "to_inspect_land_incr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_inspect_land_decr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "province_list = ['Phichit', 'Chaiyaphum']\n",
    "\n",
    "\n",
    "_, ax = plt.subplots(len(province_list),1,figsize=(10,3.5*len(province_list)))\n",
    "\n",
    "land_list = ['crop', 'forest', 'shrubland']\n",
    "\n",
    "\n",
    "trend_prov = []\n",
    "for i, prov in enumerate(province_list):\n",
    "    for land in land_list:\n",
    "        temp = df[(df['label'] == land) & (df['province'] == prov)].set_index('year')['count']\n",
    "        temp = temp.loc['2009':]\n",
    "        \n",
    "        sns.regplot( x=temp.index, y=temp.values, ax=ax[i], label=prov + ' ' + land)\n",
    "    \n",
    "    ax[i].legend(bbox_to_anchor=(1.02, 1.05), frameon=True)\n",
    "        \n",
    "#plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_inspect_land_incr['province'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "province_list = ['Rayong', 'Ranong', 'Chumphon']\n",
    "\n",
    "_, ax = plt.subplots(len(province_list),1,figsize=(10,3.5*len(province_list)))\n",
    "\n",
    "land_list = ['crop', 'forest', 'shrubland']\n",
    "\n",
    "\n",
    "trend_prov = []\n",
    "for i, prov in enumerate(province_list):\n",
    "    for land in land_list:\n",
    "        temp = df[(df['label'] == land) & (df['province'] == prov)].set_index('year')['count']\n",
    "        temp = temp.loc['2009':]\n",
    "        \n",
    "        sns.regplot( x=temp.index, y=temp.values, ax=ax[i], label=prov + ' ' + land)\n",
    "    \n",
    "    ax[i].legend(bbox_to_anchor=(1.02, 1.05), frameon=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLD Landuse Level 2 data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Landuse Level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldd_folder = '../data/landuse_idd/'\n",
    "proc_folder = '../data/landuse_l2/'\n",
    "temp_folder = ldd_folder + 'temp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob('../data/landuse_idd/*.zip')\n",
    "len(files)\n",
    "filename = files[5]\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reading file with encoding problem \n",
    "with ZipFile(filename, 'r') as zipObj:\n",
    "    # Get list of files names in zip\n",
    "    listOfiles = zipObj.namelist()\n",
    "    # Iterate over the list of file names in given list & print them\n",
    "    listOfiles = [s.encode('437').decode('iso_8859_11') for s in listOfiles]\n",
    "    for elem in zipObj.namelist():\n",
    "        print(elem)\n",
    "        if '.shp' in elem:\n",
    "            shape_filename = elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the read me file \n",
    "with ZipFile(filename) as zipObj:\n",
    "    f = zipObj.infolist()[0]\n",
    "     \n",
    "    data = zipObj.read(f)\n",
    "    print(data.decode('iso_8859_11'))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projected Coordinate System : WGS_1984_UTM_Zone_47N\n",
    "    \n",
    "http://12.105.40.62:8080/32647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract zip file \n",
    "with zipfile.ZipFile(filename,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(ldd_folder + 'unzip/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the unzip file\n",
    "unzip_folder = glob(ldd_folder + 'unzip/' + '*')[0]\n",
    "unzip_files = glob(unzip_folder + '/*')\n",
    "for file in unzip_files:\n",
    "    if '.shp' in file:\n",
    "        shape_filename = file\n",
    "print(shape_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(shape_filename, encoding='iso_8859_11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[['Lu_code','Des_en']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,1, figsize=(6,10))\n",
    "gdf['geometry'].plot(cmap='jet', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.groupby('Des_en').sum().sort_values('Shape_Area', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.groupby('Des_en').sum()['Shape_Area'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_list = []\n",
    "geo_list = []\n",
    "for land in gdf['Des_en'].unique():\n",
    "    temp = gdf[gdf['Des_en'] == land]\n",
    "    gdf_group = temp['geometry'].unary_union\n",
    "    land_list.append(land)\n",
    "    geo_list.append(gdf_group)\n",
    "    \n",
    "prov_group = gpd.GeoDataFrame({'Des_en': land_list, 'geometry': geo_list}, crs=gdf.crs)\n",
    "prov_group['Shape_Area'] = prov_group['geometry'].area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,1, figsize=(6,10))\n",
    "prov_group['geometry'].plot(cmap='jet', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov_group['Shape_Area'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to google map crs\n",
    "prov_group = prov_group.to_crs(epsg=3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_prov = prov_group['geometry'].unary_union.centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov_boundary = prov_group['geometry'].unary_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov_boundary = prov_boundary.boundary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [point for polygon in prov_boundary for point in polygon.coords[:-1]]\n",
    "points = np.array(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x_min, y_min, x_max, y_max] = prov_group['geometry'].total_bounds\n",
    "xmap_range = [x_min, x_max]\n",
    "ymap_range = [y_min, y_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_center = center_prov.x\n",
    "y_center = center_prov.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(x_range=xmap_range, y_range=ymap_range, x_axis_type=\"mercator\", y_axis_type=\"mercator\",  toolbar_location='below', \n",
    "           plot_width=500, plot_height=500, title='')\n",
    "p.add_tile(get_provider(Vendors.STAMEN_TERRAIN_RETINA))\n",
    "\n",
    "p.circle(points[:,0], points[:,1], size=1)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try saving\n",
    "\n",
    "prov_group.to_file(proc_folder + \"try.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov_group = gpd.read_file(proc_folder + \"try.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for all shape files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob('../data/landuse_idd/*.zip')\n",
    "len(files)\n",
    "filename = files[5]\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build landuse level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldd_folder = '../data/landuse_idd/'\n",
    "proc_folder = '../data/landuse_l2/'\n",
    "temp_folder = ldd_folder + 'temp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = glob(ldd_folder + \"*.zip\")\n",
    "print(len(files))\n",
    "zip_file_pairs = []\n",
    "\n",
    "empty_files = []\n",
    "good_files = []\n",
    "double_files = []\n",
    "\n",
    "for file in files:\n",
    "    # reading file with encoding problem \n",
    "    with ZipFile(file, 'r') as zipObj:\n",
    "        # Get list of files names in zip\n",
    "        # listOfiles = zipObj.namelist()\n",
    "        # Iterate over the list of file names in given list & print them\n",
    "        #listOfiles = [s.encode('437').decode('iso_8859_11') for s in listOfiles]\n",
    "        \n",
    "        year = file.split('_')[-1].split('.')[0]\n",
    "        \n",
    "        for elem in zipObj.namelist():\n",
    "             \n",
    "            if 'ReadMe' in elem:\n",
    "                readme_file = elem\n",
    "                \n",
    "        with_shape = False       \n",
    "        shape_count = 0\n",
    "        for elem in zipObj.namelist():\n",
    "             \n",
    "            if ('.shp' in elem) and ('xml' not in elem) and ('CIT') not in elem:\n",
    "                shape_filename = elem\n",
    "                zip_file_pairs.append([file, shape_filename, readme_file, year])\n",
    "                with_shape = True\n",
    "                shape_count += 1\n",
    "                \n",
    "        if with_shape:\n",
    "            good_files.append(file)\n",
    "        else:\n",
    "            empty_files.append(file)\n",
    "        \n",
    "        if shape_count > 1:\n",
    "            double_files.append(file)\n",
    "            \n",
    "             \n",
    "\n",
    "zip_file_pairs = np.array(zip_file_pairs)\n",
    "\n",
    "zip_file_pairs = pd.DataFrame(zip_file_pairs)\n",
    "print(len(zip_file_pairs))\n",
    "\n",
    "print(len(empty_files), len(good_files), len(double_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_filename.encode('437').decode('iso_8859_11') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(unzip_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[zip_filename, shape_filename, readme_filename, year] = zip_file_pairs.iloc[42].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract zip file \n",
    "with ZipFile(zip_filename, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(temp_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip folder to be delete afterward \n",
    "unzip_folder = glob(temp_folder + '/*')[0] + '/'\n",
    "\n",
    "print(unzip_folder)\n",
    "\n",
    "# prepare new shape file\n",
    "shape_filename = temp_folder + shape_filename\n",
    "\n",
    "print(shape_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[prov_name, year] = shape_filename.encode('437').decode('iso_8859_11').split('_')[-2:]\n",
    "#year = year.split('.')[0]\n",
    "\n",
    "year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read unzip file \n",
    "prov_land = gpd.read_file(shape_filename, encoding='iso_8859_11', SHAPE_RESTORE_SHX=True)\n",
    "# change crs to mercator\n",
    "prov_land = prov_land.to_crs(epsg=3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for land in prov_land['Des_en'].unique(): \n",
    "    temp = prov_land[prov_land['Des_en'] == land]\n",
    "    if len(temp) > 0:\n",
    "        #temp['geometry'] = temp['geometry'].buffer(0)\n",
    "        gdf_group = temp['geometry'].unary_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by landuse \n",
    "group_land = group_gdf_bylanduse(prov_land)\n",
    "readme_filename = temp_folder + readme_filename\n",
    "start_year, end_year = extract_year_range(readme_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract province name and year of survey\n",
    "print(shape_filename)\n",
    "[prov_name, year] = shape_filename.encode('437').decode('iso_8859_11').split('_')[-2:]\n",
    "print(year)\n",
    "\n",
    "result = re.search('(\\d{4})', year)\n",
    "year = result.group(0)\n",
    "\n",
    "print(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add meta information\n",
    "group_land['province'] = prov_name\n",
    "group_land['year'] = int(year) - 543 \n",
    "group_land['start_year'] = start_year \n",
    "group_land['end_year'] = end_year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_gdf_bylanduse(gdf, col='Des_en', to_print=False):\n",
    "    land_list = []\n",
    "    geo_list = []\n",
    "    for land in tqdm(gdf[col].unique()):\n",
    "            \n",
    "        temp = gdf[gdf[col] == land]\n",
    "        if (len(temp) > 0):\n",
    "            temp['geometry'] = temp['geometry'].buffer(0)\n",
    "            gdf_group = temp['geometry'].unary_union\n",
    "            land_list.append(land)\n",
    "            geo_list.append(gdf_group)\n",
    "    \n",
    "    return gpd.GeoDataFrame({'des_en': land_list, 'geometry': geo_list}, crs=gdf.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year_range(readme_filename):\n",
    "    \n",
    "    with open(readme_filename, encoding='iso_8859_11') as f:\n",
    "        readme_txt = f.read()\n",
    "    \n",
    "    year_range = [s for s in readme_txt.split('\\n') if 'รอบการผลิตข้อมูล' in s]\n",
    "    year_range = year_range[0]\n",
    "    \n",
    "    year_split = year_range.split(':')[-1].split('-')\n",
    "    if len(year_split) == 2:\n",
    "        [start_year, end_year] = year_split\n",
    "    elif len(year_split) ==1:\n",
    "        start_year = end_year = year_split[0]\n",
    "    start_year = int(start_year) - 543 \n",
    "    end_year = int(end_year) - 543 \n",
    "    \n",
    "    return start_year, end_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prov_group(zip_filename, shape_filename, readme_filename, ld_folder = '../data/landuse_idd/', fix_encode=True):\n",
    "    \n",
    "    temp_folder = ld_folder + 'temp/'\n",
    "    # extract zip file \n",
    "    with ZipFile(zip_filename, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(temp_folder)\n",
    "        \n",
    "    # unzip folder to be delete afterward \n",
    "    # unzip_folder = '/'.join(shape_filename.split('/')[:-1]) + '/'\n",
    "\n",
    "    # prepare new shape file\n",
    "    shape_filename = temp_folder  + shape_filename\n",
    "    # fix one bad file\n",
    "    if '25450' in shape_filename:\n",
    "        old_file = shape_filename\n",
    "        new_file = old_file.replace('25450', '2550') \n",
    "        shape_filename = new_file\n",
    "        if not os.path.exists(new_file):\n",
    "            os.rename(old_file, new_file)\n",
    "        \n",
    "        \n",
    "    # read unzip file \n",
    "    prov_land = gpd.read_file(shape_filename, encoding='iso_8859_11')\n",
    "    # make all columns lower case \n",
    "    new_col = [s.lower() for s in prov_land.columns]\n",
    "    prov_land.columns = new_col\n",
    "    \n",
    "    #print(prov_land.columns)\n",
    "    \n",
    "    if len(prov_land.columns) > 1:\n",
    "        label_col = []\n",
    "        if 'des_en' in prov_land.columns:\n",
    "            col = 'des_en'\n",
    "            label_col = [ 'des_th', 'des_en']\n",
    "         \n",
    "        elif 'lu_des_en' in prov_land.columns:\n",
    "            col = 'lu_des_en'\n",
    "            label_col = ['lu_des_th', 'lu_des_en']\n",
    "        else:\n",
    "            print(prov_land.columns)\n",
    "    \n",
    "        if 'lu_group' in prov_land.columns:\n",
    "            label_col += ['lu_group']\n",
    "        elif 'lu_groub' in prov_land.columns:\n",
    "            label_col += ['lu_groub']\n",
    "        elif 'lu_code' in prov_land.columns:\n",
    "            label_col += ['lu_code']\n",
    "         \n",
    "        else:\n",
    "            print(prov_land.columns)\n",
    "        # change crs to mercator\n",
    "        prov_land = prov_land.to_crs(epsg=3857)\n",
    "        label_df = prov_land[label_col].drop_duplicates()\n",
    "    \n",
    "        # group by landuse \n",
    "        group_land = group_gdf_bylanduse(prov_land, col=col)\n",
    "        \n",
    "        if len(readme_filename)>0:\n",
    "            # use readme to extract year \n",
    "            readme_filename = temp_folder + readme_filename\n",
    "            start_year, end_year = extract_year_range(readme_filename)\n",
    "        else:\n",
    "            start_year = end_year = np.nan \n",
    "    \n",
    "        # extract province name and year of survey\n",
    "        if fix_encode:\n",
    "            [prov_name, year] = shape_filename.encode('437').decode('iso_8859_11').split('_')[-2:]\n",
    "            result = re.search('(\\d{4})', year)\n",
    "            year = result.group(0)\n",
    "        else:\n",
    "            result = re.search('/(\\D+)(\\d{4})/',shape_filename)\n",
    "            prov = result.group(1)\n",
    "            year = result.group(2)\n",
    "            \n",
    "        \n",
    "        if year == '2545':\n",
    "            year = '2550'\n",
    "    \n",
    "        # add meta information\n",
    "        group_land['province'] = prov_name\n",
    "        group_land['year'] = int(year) - 543 \n",
    "        group_land['start_year'] = start_year \n",
    "        group_land['end_year'] = end_year \n",
    "    \n",
    "    else:\n",
    "        group_land = pd.DataFrame()\n",
    "        label_df = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    return group_land, label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_pairs[3].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all = []\n",
    "\n",
    "for year in ['2562', '2560' ]:\n",
    "    \n",
    "    print(year)\n",
    "    \n",
    "    save_folder = proc_folder + str(year) + '/'\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.mkdir(save_folder)\n",
    "    \n",
    "    save_filename = save_folder + str(year) + '.shp'\n",
    "    \n",
    "    # select file for that year\n",
    "    temp = zip_file_pairs[zip_file_pairs[3] == year]\n",
    "    \n",
    "    year_gdf = []\n",
    "    \n",
    "    \n",
    "    for i, row in tqdm(temp.iterrows()):\n",
    "    \n",
    "        [zip_filename, shape_filename, readme_filename, year] = row.to_list()\n",
    "        \n",
    "        # print(zip_filename)\n",
    "        \n",
    "        group_gdf, label_df = build_prov_group(zip_filename, shape_filename, readme_filename, ld_folder=ldd_folder )\n",
    "        year_gdf.append(group_gdf)\n",
    "        label_all.append(label_df)\n",
    "        \n",
    "    # remove unzip files\n",
    "    rm_files = glob(ldd_folder +  'temp/' + '*')\n",
    "    # remove unzip files \n",
    "    for rm_file in rm_files:\n",
    "        try:\n",
    "            shutil.rmtree(rm_file)\n",
    "        except OSError:\n",
    "            os.remove(rm_file)  \n",
    "    \n",
    "    if len(year_gdf) > 0:\n",
    "        \n",
    "        year_gdf = pd.concat(year_gdf, ignore_index=True)\n",
    "        year_gdf.to_file(save_filename, encoding = 'utf-8')\n",
    "\n",
    "label_all = pd.concat(label_all, ignore_index=True)\n",
    "label_all.to_csv(proc_folder + 'level2_labels.csv', index=False, encoding = 'utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_gdf = gpd.read_file(save_filename, encoding = 'utf-8')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_gdf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_label = pd.read_csv(proc_folder + 'level2_labels.csv', encoding = 'utf-8')\n",
    "l2_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_label.to_csv(proc_folder + 'level2_labels.csv', encoding = 'utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to replace + with / for all of these columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data to remove province information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(proc_folder + '*prov/*.shp')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build all pollution data\n",
    "mdataset = MapDataset('Thailand')\n",
    "mdataset.load_()\n",
    "prov_dict = mdataset.prov_map[['admin1Name_th', 'province']]\n",
    "prov_dict.columns = ['prov_th', 'province']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_prov(s):\n",
    "    replace_dict = {'จ.':'',\n",
    "                    'จ.จันทบุรี':'จันทบุรี',\n",
    "                    'จ.ชัยนาท': 'ชัยนาท',\n",
    "                    'จ.ปราจีนบุรี': 'ปราจีนบุรี',\n",
    "                    'จ.พระนครศรีอยุธยา': 'พระนครศรีอยุธยา',\n",
    "                    'จ.เพชรบูรณ์': 'เพชรบูรณ์',\n",
    "                   'กาญบุรี': 'กาญจนบุรี',\n",
    "                   'นทบุรี':   'นนทบุรี',\n",
    "                    'ประบคีรีขันธ์': 'ประจวบคีรีขันธ์', \n",
    "                    'ปรานบุรี': 'ปราจีนบุรี', \n",
    "                    'พิตร': 'พิจิตร', \n",
    "                    'อำนาิญ': 'อำนาจเจริญ',\n",
    "                    'แม่ฮองสอน': 'แม่ฮ่องสอน'\n",
    "                   }\n",
    "    s = s.replace(replace_dict)\n",
    "    #s = s.str.replace('กาญบุรี', 'กาญจนบุรี')\n",
    "    #s = s.str.replace('กาญบุรี', 'กาญจนบุรี')\n",
    "    #s = s.str.replace('นทบุรี', )\n",
    "    s = s.str.lstrip()\n",
    "    s = s.str.rstrip()\n",
    "    return s\n",
    "\n",
    "def proc_label(s):\n",
    "    s = s.str.replace('+', '/')\n",
    "    s = s.str.replace('Integrated farm/ Diversified farm', 'Integrated farm/Diversified farm')\n",
    "    return s.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gdf0 = gpd.read_file(files[2])\n",
    "gdf1 = gpd.read_file(files[3])\n",
    "gdf2 = gpd.read_file(files[4])\n",
    "gdf3 = gpd.read_file(files[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gdf0['province'] = proc_prov(gdf0['province'] )\n",
    "gdf1['province'] = proc_prov(gdf1['province'] )\n",
    "gdf2['province'] = proc_prov(gdf2['province'] )\n",
    "gdf3['province'] = proc_prov(gdf3['province'] )\n",
    "\n",
    "name_dict = {'province':'prov_th'}\n",
    "#gdf0 = gdf0.rename(columns= name_dict)\n",
    "gdf1 = gdf1.rename(columns= name_dict)\n",
    "gdf2 = gdf2.rename(columns= name_dict)\n",
    "gdf3 = gdf3.rename(columns= name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gdf0['des_en'] =  proc_label(gdf0['des_en'])\n",
    "gdf1['des_en'] =  proc_label(gdf1['des_en'])\n",
    "gdf2['des_en'] =  proc_label(gdf2['des_en'])\n",
    "gdf3['des_en'] =  proc_label(gdf3['des_en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf1['prov_th'] = proc_prov(gdf1['prov_th'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf1['prov_th'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = gdf2[~gdf2['prov_th'].isin(gdf1['prov_th'].unique())]\n",
    "print(temp['prov_th'].unique())\n",
    "gdf1 = pd.concat([gdf1, temp], ignore_index=True)\n",
    "\n",
    "temp = gdf3[~gdf3['prov_th'].isin(gdf1['prov_th'].unique())]\n",
    "print(temp['prov_th'].unique())\n",
    "gdf1 = pd.concat([gdf1, temp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf1['prov_th'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_gdf1 = gdf1.merge(prov_dict, on='prov_th', how='left')\n",
    "new_gdf1[new_gdf1['province'].isna()]['prov_th'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2553'\n",
    "year = str(int(year) - 543 )\n",
    "\n",
    "save_folder = proc_folder + year + '/'\n",
    "try:\n",
    "    os.mkdir(save_folder)\n",
    "except:\n",
    "    pass\n",
    "save_filename = save_folder + year + '.shp'\n",
    "print(save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gdf1['year'] = year\n",
    "new_gdf1[['des_en', 'year', 'geometry', 'province']].to_file(save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2551'\n",
    "year = str(int(year) - 543 )\n",
    "\n",
    "save_folder = proc_folder + year + '/'\n",
    "try:\n",
    "    os.mkdir(save_folder)\n",
    "except:\n",
    "    pass\n",
    "save_filename = save_folder + year + '.shp'\n",
    "print(save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = gdf1[~gdf1['prov_th'].isin(gdf2['prov_th'].unique())]\n",
    "print(temp['prov_th'].unique())\n",
    "gdf2 = pd.concat([gdf2, temp], ignore_index=True)\n",
    "\n",
    "temp = gdf3[~gdf3['prov_th'].isin(gdf2['prov_th'].unique())]\n",
    "print(temp['prov_th'].unique())\n",
    "gdf2 = pd.concat([gdf2, temp], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf2['prov_th'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf2['prov_th'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf2['prov_th'] = proc_prov(gdf2['prov_th'] )\n",
    "new_gdf2 = gdf2.merge(prov_dict, on='prov_th', how='left')\n",
    "new_gdf2[new_gdf2['province'].isna()]['prov_th'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_gdf2[['des_en', 'year', 'geometry', 'province']].to_file(save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = gdf1[~gdf1['prov_th'].isin(gdf3['prov_th'].unique())]\n",
    "print(temp['prov_th'].unique())\n",
    "gdf3 = pd.concat([gdf3, temp], ignore_index=True)\n",
    "\n",
    "temp = gdf2[~gdf2['prov_th'].isin(gdf3['prov_th'].unique())]\n",
    "print(temp['prov_th'].unique())\n",
    "gdf3 = pd.concat([gdf3, temp], ignore_index=True)\n",
    "\n",
    "print(gdf3['prov_th'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf3['prov_th'] = proc_prov(gdf3['prov_th'] )\n",
    "new_gdf3 = gdf3.merge(prov_dict, on='prov_th', how='left')\n",
    "new_gdf3[new_gdf3['province'].isna()]['prov_th'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2552'\n",
    "year = str(int(year) - 543 )\n",
    "\n",
    "save_folder = proc_folder + year + '/'\n",
    "try:\n",
    "    os.mkdir(save_folder)\n",
    "except:\n",
    "    pass\n",
    "save_filename = save_folder + year + '.shp'\n",
    "print(save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_gdf3[['des_en', 'year', 'geometry', 'province']].to_file(save_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Label 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../data/landuse_l2/level2_labels.csv'\n",
    "label2 = pd.read_csv(file)\n",
    "label2 = label2.dropna()\n",
    "label2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2['lu_code'] = label2['lu_code'].str.replace('+', '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2 = label2[(~label2['lu_code'].str.contains('/').fillna(True)) ]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLD Level 3 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Level 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldd3_folder = '../data/landuse_idd_level3/'\n",
    "ldd3temp_folder = ldd3_folder + 'temp/'\n",
    "proc3_folder = '../data/landuse_l3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(ldd3_folder + '*.zip')\n",
    "len(files)\n",
    "filename = files[5]\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading file with encoding problem \n",
    "with ZipFile(filename, 'r') as zipObj:\n",
    "    # Get list of files names in zip\n",
    "    for elem in zipObj.namelist():\n",
    "        print(elem)\n",
    "        if '.shp' in elem:\n",
    "            shape_filename = elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract zip file \n",
    "with ZipFile(filename,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(ldd3temp_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(ldd3temp_folder + '*/*/')\n",
    "files = glob(files[0] + '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_file = files[0]\n",
    "# possible to have more than one file in this folder \n",
    "shape_file = glob(files[1] + '/*.shp')[0] \n",
    "print(class_file)\n",
    "print(shape_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read classification files\n",
    "# don't need \n",
    "df = pd.read_excel(class_file, header=1)\n",
    "# rename col \n",
    "df = df.rename(columns = {'Unnamed: 2': \"level2_label\", 'Unnamed: 4': 'level3_label_th', 'Unnamed: 5': 'level4_label'})\n",
    "idx = df[df.iloc[:,0].str.contains('หมายเหตุ').fillna(False)].index[0]\n",
    "df = df.iloc[:idx,:]\n",
    "df = df.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(shape_file, encoding='iso_8859_11')\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.groupby('DES_EN').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_land = group_gdf_bylanduse(gdf, col='DES_EN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_land['area'] =  group_land['geometry'].area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_land.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_land.sort_values('Des_en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to google map crs\n",
    "\n",
    "prov_group = group_land.to_crs(epsg=3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_prov = prov_group['geometry'].unary_union.centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov_boundary = prov_group['geometry'].unary_union\n",
    "prov_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#points = [point for polygon in prov_boundary for point in polygon.coords[:-1]]\n",
    "points = prov_boundary.boundary.coords[:-1]\n",
    "points = np.array(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x_min, y_min, x_max, y_max] = prov_group['geometry'].total_bounds\n",
    "xmap_range = [x_min, x_max]\n",
    "ymap_range = [y_min, y_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_center = center_prov.x\n",
    "y_center = center_prov.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(x_range=xmap_range, y_range=ymap_range, x_axis_type=\"mercator\", y_axis_type=\"mercator\",  toolbar_location='below', \n",
    "           plot_width=500, plot_height=500, title='')\n",
    "p.add_tile(get_provider(Vendors.STAMEN_TERRAIN_RETINA))\n",
    "\n",
    "p.circle(points[:,0], points[:,1], size=1)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = zip_file_pairs[zip_file_pairs[3] == '2559']\n",
    "temp.iloc[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[zip_filename, shape_filename, readme_filename, year] = zip_file_pairs.iloc[348].to_list()\n",
    "print(shape_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract zip file \n",
    "with ZipFile(zip_filename, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(ldd3temp_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip folder to be delete afterward \n",
    "unzip_folder = glob(ldd3temp_folder + '/*')[0] + '/'\n",
    "\n",
    "print(unzip_folder)\n",
    "\n",
    "# prepare new shape file\n",
    "shape_filename = ldd3temp_folder + shape_filename\n",
    "\n",
    "print(shape_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_object = re.search('/(\\D+)(\\d{4})/',shape_filename)\n",
    "prov = match_object.group(1)\n",
    "year = match_object.group(2)\n",
    "\n",
    "print(prov, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read unzip file \n",
    "prov_land = gpd.read_file(shape_filename, encoding='iso_8859_11')\n",
    "\n",
    "if prov_land.crs == None:\n",
    "    print('no crs')\n",
    "    crs = pyproj.CRS(\"EPSG:32647\")\n",
    "    prov_land.crs = crs\n",
    "    \n",
    "# change crs to mercator\n",
    "prov_land = prov_land.to_crs(epsg=3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov_land.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_gdf, label_df, unzip_folder = build_prov_group3(zip_filename, shape_filename, readme_filename, ld_folder=ldd3_folder, fix_encode=False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build land use level 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldd3_folder = '../data/landuse_idd_level3/'\n",
    "ldd3temp_folder = ldd3_folder + 'temp/'\n",
    "proc3_folder = '../data/landuse_l3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prov_group3(zip_filename, shape_filename, readme_filename, ld_folder = '../data/landuse_idd/', fix_encode=True, unzip_folder=[]):\n",
    "    \n",
    "    temp_folder = ld_folder + 'temp/'\n",
    "    \n",
    "    if zip_filename not in unzip_folder:\n",
    "        unzip_folder.append(zip_filename)\n",
    "        # extract zip file \n",
    "        with ZipFile(zip_filename, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(temp_folder)\n",
    "        \n",
    "    # unzip folder to be delete afterward \n",
    "    # unzip_folder = '/'.join(shape_filename.split('/')[:-1]) + '/'\n",
    "\n",
    "    # prepare new shape file\n",
    "    shape_filename = temp_folder  + shape_filename\n",
    "     \n",
    "        \n",
    "    # read unzip file \n",
    "    prov_land = gpd.read_file(shape_filename, encoding='iso_8859_11')\n",
    "    # make all columns lower case \n",
    "    new_col = [s.lower() for s in prov_land.columns]\n",
    "    prov_land.columns = new_col\n",
    "    \n",
    "    #print(prov_land.columns)\n",
    "    \n",
    "    if len(prov_land.columns) > 1:\n",
    "        label_col = []\n",
    "        \n",
    "        if 'lu_group' in prov_land.columns:\n",
    "            label_col += ['lu_group']\n",
    "            col = 'lu_group'\n",
    "        elif 'lu_groub' in prov_land.columns:\n",
    "            label_col += ['lu_groub']\n",
    "            col = 'lu_groub'\n",
    "        elif 'lu_code' in prov_land.columns:\n",
    "            label_col += ['lu_code']\n",
    "            col = 'lu_code'\n",
    "        elif 'lucode' in prov_land.columns:\n",
    "            label_col += ['lucode']\n",
    "            col = 'lucode'\n",
    "        elif 'lucode_51' in prov_land.columns:\n",
    "            label_col += ['lucode_51']\n",
    "            col = 'lucode_51'\n",
    "        elif 'lucode_52' in prov_land.columns:\n",
    "            label_col += ['lucode_52']\n",
    "            col = 'lucode_52'\n",
    "        else:\n",
    "            print(prov_land.columns)\n",
    "            \n",
    "        if 'des_th' in prov_land.columns:\n",
    "            label_col += ['des_th']\n",
    "        elif 'des_th51' in prov_land.columns:\n",
    "            label_col += ['des_th51']\n",
    "        elif 'des_th_52' in prov_land.columns:\n",
    "            label_col += ['des_th_52']\n",
    "        \n",
    "        if 'des_en' in prov_land.columns:\n",
    "            label_col += ['des_en']\n",
    "        elif 'des_en51' in prov_land.columns:\n",
    "            label_col += ['des_en51']\n",
    "        elif 'des_en_52' in prov_land.columns:\n",
    "            label_col += ['des_en_52']\n",
    "           \n",
    "        # change crs to mercator\n",
    "        if prov_land.crs == None:\n",
    "            crs = pyproj.CRS(\"EPSG:32647\")\n",
    "            prov_land.crs = crs\n",
    "            \n",
    "        prov_land = prov_land.to_crs(epsg=3857)\n",
    "        label_df = prov_land[label_col].drop_duplicates()\n",
    "    \n",
    "        # group by landuse \n",
    "        group_land = group_gdf_bylanduse(prov_land, col=col)\n",
    "        \n",
    "        if len(readme_filename)>0:\n",
    "            # use readme to extract year \n",
    "            readme_filename = temp_folder + readme_filename\n",
    "            start_year, end_year = extract_year_range(readme_filename)\n",
    "        else:\n",
    "            start_year = end_year = np.nan \n",
    "    \n",
    "        # extract province name and year of survey\n",
    "        if fix_encode:\n",
    "            [prov_name, year] = shape_filename.encode('437').decode('iso_8859_11').split('_')[-2:]\n",
    "            result = re.search('(\\d{4})', year)\n",
    "            year = result.group(0)\n",
    "        else:\n",
    "            if shape_filename == '../data/landuse_idd_level3/temp/ภาคใต้/กระบี่252/การใช้ที่ดิน/lu_krabi_2552 update.shp':\n",
    "                prov_name = 'กระบี่'\n",
    "                year = '2552'\n",
    "            else:\n",
    "                result = re.search('/(\\D+)(\\d{4})/',shape_filename)\n",
    "                prov_name = result.group(1)\n",
    "                year = result.group(2)\n",
    "            \n",
    "        \n",
    "        # add meta information\n",
    "        group_land['province'] = prov_name\n",
    "        group_land['year'] = int(year) - 543 \n",
    "        group_land['start_year'] = start_year \n",
    "        group_land['end_year'] = end_year \n",
    "    \n",
    "    else:\n",
    "        group_land = pd.DataFrame()\n",
    "        label_df = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    return group_land, label_df, unzip_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for all zip files\n",
    "files = glob(ldd3_folder + '*.zip')\n",
    "print(len(files))\n",
    "zip_file_pairs = []\n",
    "\n",
    "empty_files = []\n",
    "good_files = []\n",
    "double_files = []\n",
    "\n",
    "# look for all shape files \n",
    "for file in files:\n",
    "    # reading file with encoding problem\n",
    "    year = re.search('(\\d{4})', file).group(0)\n",
    "    \n",
    "    with ZipFile(file, 'r') as zipObj:\n",
    "        \n",
    "        with_shape = False       \n",
    "        shape_count = 0\n",
    "        for elem in zipObj.namelist():\n",
    "             \n",
    "            if ('.shp' in elem) and ('xml' not in elem) and ('การใช้ที่ดิน' in elem) and ('.shp.' not in elem):\n",
    "                shape_filename = elem\n",
    "                 \n",
    "                zip_file_pairs.append([file, shape_filename, '', year])\n",
    "                with_shape = True\n",
    "                shape_count += 1\n",
    "        \n",
    "        if with_shape:\n",
    "            good_files.append(file)\n",
    "        else:\n",
    "            empty_files.append(file)\n",
    "        \n",
    "        if shape_count > 1:\n",
    "            double_files.append(file)\n",
    "            \n",
    "zip_file_pairs = np.array(zip_file_pairs)\n",
    "\n",
    "zip_file_pairs = pd.DataFrame(zip_file_pairs)\n",
    "print(len(zip_file_pairs))\n",
    "\n",
    "print(len(empty_files), len(good_files), len(double_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_pairs[3].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all = []\n",
    "\n",
    "for year in ['2550']:\n",
    "    \n",
    "    save_folder = proc3_folder + str(year) + '_prov/'\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.mkdir(save_folder)\n",
    "    \n",
    "    save_filename = save_folder + str(year) + '.shp'\n",
    "    print(save_filename)\n",
    "    \n",
    "    # select file for that year\n",
    "    temp = zip_file_pairs[zip_file_pairs[3] == year]\n",
    "    print(year + ' has length ', len(temp))\n",
    "    \n",
    "    year_gdf = []\n",
    "    unzip_folder = []\n",
    "    bad_row = []\n",
    "    \n",
    "    \n",
    "    for i, row in tqdm(temp.iterrows()):\n",
    "    \n",
    "        [zip_filename, shape_filename, readme_filename, year] = row.to_list()\n",
    "        \n",
    "        # print(zip_filename)\n",
    "        try:\n",
    "            group_gdf, label_df, unzip_folder = build_prov_group3(zip_filename, shape_filename, readme_filename, ld_folder=ldd3_folder, fix_encode=False, unzip_folder= unzip_folder)\n",
    "        except:\n",
    "            print('bad row ', i)\n",
    "            bad_row.append(i)\n",
    "        else:\n",
    "            label_df['year'] = int(year)\n",
    "            year_gdf.append(group_gdf)\n",
    "            label_all.append(label_df)\n",
    "        \n",
    "    # remove unzip files\n",
    "    rm_files = glob(ldd3_folder +  'temp/' + '*')\n",
    "    # remove unzip files \n",
    "    for rm_file in rm_files:\n",
    "        try:\n",
    "            shutil.rmtree(rm_file)\n",
    "        except OSError:\n",
    "            os.remove(rm_file)  \n",
    "    \n",
    "    if len(year_gdf) > 0:\n",
    "        \n",
    "        year_gdf = pd.concat(year_gdf, ignore_index=True)\n",
    "        year_gdf.to_file(save_filename, encoding = 'utf-8')\n",
    "\n",
    "label_all = pd.concat(label_all, ignore_index=True)\n",
    "label_all.to_csv(proc3_folder + 'level3_labels.csv', index=False, encoding = 'utf-8')\n",
    "\n",
    "print(bad_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "label_all = []\n",
    "\n",
    "for year in ['2551', '2552', '2553']:\n",
    "    \n",
    "    save_folder = proc3_folder + str(year) + '_prov/'\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.mkdir(save_folder)\n",
    "    \n",
    "    save_filename = save_folder + str(year) + '.shp'\n",
    "    print(save_filename)\n",
    "    \n",
    "    # select file for that year\n",
    "    temp = zip_file_pairs[zip_file_pairs[3] == year]\n",
    "    print(year + ' has length ', len(temp))\n",
    "    \n",
    "    year_gdf = []\n",
    "    unzip_folder = []\n",
    "    bad_row = []\n",
    "    \n",
    "    \n",
    "    for i, row in tqdm(temp.iterrows()):\n",
    "    \n",
    "        [zip_filename, shape_filename, readme_filename, year] = row.to_list()\n",
    "        \n",
    "        # print(zip_filename)\n",
    "        try:\n",
    "            group_gdf, label_df, unzip_folder = build_prov_group3(zip_filename, shape_filename, readme_filename, ld_folder=ldd3_folder, fix_encode=False, unzip_folder= unzip_folder)\n",
    "        except:\n",
    "            bad_row.append(i)\n",
    "        else:\n",
    "            label_df['year'] = int(year)\n",
    "            year_gdf.append(group_gdf)\n",
    "            label_all.append(label_df)\n",
    "        \n",
    "    # remove unzip files\n",
    "    rm_files = glob(ldd3_folder +  'temp/' + '*')\n",
    "    # remove unzip files \n",
    "    for rm_file in rm_files:\n",
    "        try:\n",
    "            shutil.rmtree(rm_file)\n",
    "        except OSError:\n",
    "            os.remove(rm_file)  \n",
    "    \n",
    "    if len(year_gdf) > 0:\n",
    "        \n",
    "        year_gdf = pd.concat(year_gdf, ignore_index=True)\n",
    "        year_gdf.to_file(save_filename, encoding = 'utf-8')\n",
    "\n",
    "label_all = pd.concat(label_all, ignore_index=True)\n",
    "label_all.to_csv(proc3_folder + 'level3_labels.csv', index=False, encoding = 'utf-8')\n",
    "\n",
    "print(bad_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all = []\n",
    "\n",
    "#for year in ['2554', '2555', '2556', '2558',\n",
    "#       '2559', '2560', '2561', '2562', '2563']:\n",
    "\n",
    "for year in [ '2559', '2561', '2562' ]:\n",
    "    \n",
    "    save_folder = proc3_folder + str(year) + '_prov/'\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.mkdir(save_folder)\n",
    "    \n",
    "    save_filename = save_folder + str(year) + '.shp'\n",
    "    print(save_filename)\n",
    "    \n",
    "    # select file for that year\n",
    "    temp = zip_file_pairs[zip_file_pairs[3] == year]\n",
    "    print(year + ' has length ', len(temp))\n",
    "    \n",
    "    year_gdf = []\n",
    "    unzip_folder = []\n",
    "    bad_row = []\n",
    "    \n",
    "    \n",
    "    for i, row in tqdm(temp.iterrows()):\n",
    "    \n",
    "        [zip_filename, shape_filename, readme_filename, year] = row.to_list()\n",
    "        \n",
    "        # print(zip_filename)\n",
    "        try:\n",
    "            group_gdf, label_df, unzip_folder = build_prov_group3(zip_filename, shape_filename, readme_filename, ld_folder=ldd3_folder, fix_encode=False, unzip_folder= unzip_folder)\n",
    "        except:\n",
    "            print(year, ' bad row ', i)\n",
    "            bad_row.append(i)\n",
    "        else:\n",
    "            label_df['year'] = int(year)\n",
    "            year_gdf.append(group_gdf)\n",
    "            label_all.append(label_df)\n",
    "        \n",
    "    # remove unzip files \n",
    "    rm_files = glob(ldd3_folder +  'temp/' + '*')\n",
    "    # remove unzip files \n",
    "    for rm_file in rm_files:\n",
    "        try:\n",
    "            shutil.rmtree(rm_file)\n",
    "        except OSError:\n",
    "            os.remove(rm_file)  \n",
    "    \n",
    "    if len(year_gdf) > 0:\n",
    "        \n",
    "        year_gdf = pd.concat(year_gdf, ignore_index=True)\n",
    "        year_gdf.to_file(save_filename, encoding = 'utf-8')\n",
    "\n",
    "label_all = pd.concat(label_all, ignore_index=True)\n",
    "label_all.to_csv(proc3_folder + 'level3_labels.csv', index=False, encoding = 'utf-8')\n",
    "\n",
    "print(bad_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_label = pd.read_csv(proc3_folder + 'level3_labels.csv', encoding = 'utf-8')\n",
    "l3_label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldd3_folder = '../data/landuse_idd_level3/'\n",
    "ldd3temp_folder = ldd3_folder + 'temp/'\n",
    "proc3_folder = '../data/landuse_l3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for all zip files\n",
    "files = glob(ldd3_folder + '*.zip')\n",
    "print(len(files))\n",
    "zip_file_pairs = []\n",
    "\n",
    "\n",
    "# look for all shape files \n",
    "for file in files:\n",
    "    # reading file with encoding problem\n",
    "    year = re.search('(\\d{4})', file).group(0)\n",
    "    \n",
    "    with ZipFile(file, 'r') as zipObj:\n",
    "        \n",
    "        with_shape = False       \n",
    "        shape_count = 0\n",
    "        for elem in zipObj.namelist():\n",
    "            if ('.xls' in elem) and (('Lu_class' in elem) or ('LUCODE'in elem) or ('LU_CODE' in elem) or ('Lu_class' in elem)):\n",
    "                shape_filename = elem\n",
    "                 \n",
    "                zip_file_pairs.append([file, shape_filename, year])\n",
    "                with_shape = True\n",
    "                shape_count += 1\n",
    "        \n",
    "        \n",
    "            \n",
    "zip_file_pairs = np.array(zip_file_pairs)\n",
    "\n",
    "zip_file_pairs = pd.DataFrame(zip_file_pairs)\n",
    "print(len(zip_file_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_zip(ldd3_folder, zip_filename, unzip_folder=[]):\n",
    "    temp_folder = ldd3_folder + 'temp/'\n",
    "    \n",
    "    if zip_filename not in unzip_folder:\n",
    "        unzip_folder.append(zip_filename)\n",
    "        # extract zip file \n",
    "        with ZipFile(zip_filename, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(temp_folder)\n",
    "    return temp_folder, unzip_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_pairs[2].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unzip_folder = []\n",
    "label_files = []\n",
    "for year in zip_file_pairs[2].unique():\n",
    "    label_year = []\n",
    "    \n",
    "    # select file for that year\n",
    "    temp = zip_file_pairs[zip_file_pairs[2] == year]\n",
    "    print(year + ' has length ', len(temp))\n",
    "    for i, row in temp.iterrows():\n",
    "        [zip_filename, excel_filename, year] = row.to_list()\n",
    "        temp_folder, unzip_folder = unpack_zip(ldd3_folder, zip_filename, unzip_folder)\n",
    "        excel_filename = temp_folder + excel_filename   \n",
    "        label = pd.read_excel(excel_filename, header=None)\n",
    "        label_year.append(label)\n",
    "    \n",
    "    label_year = pd.concat(label_year)\n",
    "    label_year = label_year.drop_duplicates()\n",
    "    filename = proc3_folder + 'label_' + year + '.csv'\n",
    "    label_files.append(filename)\n",
    "    label_year.to_csv(filename, index=False)\n",
    "    \n",
    "    # remove unzip files \n",
    "    rm_files = glob(ldd3_folder +  'temp/' + '*')\n",
    "    # remove unzip files \n",
    "    for rm_file in rm_files:\n",
    "        try:\n",
    "            shutil.rmtree(rm_file)\n",
    "        except:\n",
    "            try:\n",
    "                os.remove(rm_file)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "print(label_files)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_files = ['../data/landuse_l3/label_2550.csv', '../data/landuse_l3/label_2551.csv', '../data/landuse_l3/label_2552.csv', '../data/landuse_l3/label_2553.csv', '../data/landuse_l3/label_2554.csv', '../data/landuse_l3/label_2555.csv', '../data/landuse_l3/label_2556.csv', '../data/landuse_l3/label_2558.csv', '../data/landuse_l3/label_2559.csv', '../data/landuse_l3/label_2560.csv', '../data/landuse_l3/label_2561.csv', '../data/landuse_l3/label_2562.csv', '../data/landuse_l3/label_2563.csv']\n",
    "print(len(label_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up each files \n",
    "filename = label_files[5]\n",
    "print(filename)\n",
    "label = pd.read_csv(filename).dropna(axis=1, how='all')\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = label.iloc[:, 3:6].dropna()\n",
    "label.columns = ['lucode', 'des_th', 'des_en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in label.columns:\n",
    "    label[col] = label[col].str.lstrip()\n",
    "    label[col] = label[col].str.rstrip()\n",
    "    label[col] = label[col].str.replace('*','')\n",
    "    label[col] = label[col].str.replace('  ', ' ')\n",
    "    print(label[col].nunique())\n",
    "\n",
    "label['des_en'] = label['des_en'].str.lower()\n",
    "label = label[label['des_en'] != '-']\n",
    "label = label.drop_duplicates()\n",
    "print(label.shape)\n",
    "label = label.sort_values(['lucode', 'des_en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_to_check = label['lucode'].value_counts()[label['lucode'].value_counts() > 1].index\n",
    "for code in code_to_check:\n",
    "    print(label[label['lucode'] == code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = label.drop_duplicates('lucode', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in label.columns:\n",
    "    print(label[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_to_check = label['des_th'].value_counts()[label['des_th'].value_counts() > 1].index\n",
    "for code in code_to_check:\n",
    "    print(label[label['des_th'] == code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_to_check = label['des_en'].value_counts()[label['des_en'].value_counts() > 1].index\n",
    "for code in code_to_check:\n",
    "    print(label[label['des_en'] == code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine labels from all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_files = ['../data/landuse_l3/label_2550.csv', '../data/landuse_l3/label_2551.csv', '../data/landuse_l3/label_2552.csv', '../data/landuse_l3/label_2553.csv', '../data/landuse_l3/label_2554.csv', '../data/landuse_l3/label_2555.csv', '../data/landuse_l3/label_2556.csv', '../data/landuse_l3/label_2558.csv', '../data/landuse_l3/label_2559.csv', '../data/landuse_l3/label_2560.csv', '../data/landuse_l3/label_2561.csv', '../data/landuse_l3/label_2562.csv', '../data/landuse_l3/label_2563.csv']\n",
    "print(len(label_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all = []\n",
    "for file in label_files:\n",
    "    label = pd.read_csv(file)\n",
    "    print(file, label.columns)\n",
    "    label_all.append(label)\n",
    "    \n",
    "label_all = pd.concat(label_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all = label_all.drop_duplicates()\n",
    "label_all.to_csv('../data/landuse_l3/label_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all = label_all.sort_values('lucode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in label_all.columns:\n",
    "    print(label_all[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_to_check = label_all['lucode'].value_counts()[label_all['lucode'].value_counts() > 1].index\n",
    "for code in code_to_check:\n",
    "    print(label_all[label_all['lucode'] == code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all = label_all.drop_duplicates('lucode', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_en_dict = {'scrub': 'shrubland',\n",
    "              'resort, hotel, guesthouse': 'resort, hotel, guesthouse, golf course',\n",
    "              'abandoned perenial': 'abandoned perennial',\n",
    "              'marihuana': 'marijuana, hump',\n",
    "              'abandoned institutional land': 'institutional land'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all['des_en'] = label_all['des_en'].replace(des_en_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in label.columns:\n",
    "    print(label[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_to_check = label_all['des_th'].value_counts()[label_all['des_th'].value_counts() > 1].index\n",
    "for code in code_to_check:\n",
    "    print(label_all[label_all['des_th'] == code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all.to_csv('../data/landuse_l3/label_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldd3_folder = '../data/landuse_idd_level3/'\n",
    "ldd3temp_folder = ldd3_folder + 'temp/'\n",
    "proc3_folder = '../data/landuse_l3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(proc3_folder + '*prov/*.shp')\n",
    "print(len(files))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build all pollution data\n",
    "mdataset = MapDataset('Thailand')\n",
    "mdataset.load_()\n",
    "prov_dict = mdataset.prov_map[['admin1Name_th', 'province']]\n",
    "prov_dict.columns = ['prov_th', 'province']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_prov_l3(s):\n",
    "    \n",
    "    new_s = [string.split('/')[-1] for string in s]\n",
    "    new_s = pd.Series(new_s, index=s.index)\n",
    "    \n",
    "    replace_dict = {'จ.':'',\n",
    "                    'จ.จันทบุรี':'จันทบุรี',\n",
    "                    'จ.ชัยนาท': 'ชัยนาท',\n",
    "                    'จ.ปราจีนบุรี': 'ปราจีนบุรี',\n",
    "                    'จ.พระนครศรีอยุธยา': 'พระนครศรีอยุธยา',\n",
    "                    'จ.เพชรบูรณ์': 'เพชรบูรณ์',\n",
    "                   'กาญบุรี': 'กาญจนบุรี',\n",
    "                   'นทบุรี':   'นนทบุรี',\n",
    "                    'ประบคีรีขันธ์': 'ประจวบคีรีขันธ์', \n",
    "                    'ปรานบุรี': 'ปราจีนบุรี', \n",
    "                    'พิตร': 'พิจิตร', \n",
    "                    'อำนาิญ': 'อำนาจเจริญ',\n",
    "                    'แม่ฮองสอน': 'แม่ฮ่องสอน',\n",
    "                    'กรุงเทพฯ': 'กรุงเทพมหานคร',\n",
    "                    'สุมทรสงคราม': 'สมุทรสงคราม',\n",
    "                    'สิงหบุรี': 'สิงห์บุรี',\n",
    "                    'สุราษฏร์ธานี': 'สุราษฎร์ธานี',\n",
    "                    'กาฬสินธ์':'กาฬสินธุ์',\n",
    "                    'อำนาญเจริญ':'อำนาจเจริญ'\n",
    "                   }\n",
    "    new_s = new_s.replace(replace_dict)\n",
    "    #s = s.str.replace('กาญบุรี', 'กาญจนบุรี')\n",
    "    #s = s.str.replace('กาญบุรี', 'กาญจนบุรี')\n",
    "    #s = s.str.replace('นทบุรี', )\n",
    "    new_s = new_s.str.lstrip()\n",
    "    new_s = new_s.str.rstrip()\n",
    "    return new_s\n",
    "\n",
    "def proc_label_l3(s):\n",
    "    s = s.str.replace('+', '/')\n",
    "    s = s.str.replace('Integrated farm/ Diversified farm', 'Integrated farm/Diversified farm')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#gdf0 = gpd.read_file(files[2])\n",
    "i = 12\n",
    "print(files[i])\n",
    "gdf1 = gpd.read_file(files[i]).drop(['start_year', 'end_year'], axis=1)\n",
    "gdf2 = gpd.read_file(files[i-1]).drop(['start_year', 'end_year'], axis=1)\n",
    "gdf3 = gpd.read_file(files[i-2]).drop(['start_year', 'end_year'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gdf0['province'] = proc_prov(gdf0['province'] )\n",
    "gdf1['province'] = proc_prov_l3(gdf1['province'] )\n",
    "gdf2['province'] = proc_prov_l3(gdf2['province'] )\n",
    "gdf3['province'] = proc_prov_l3(gdf3['province'] )\n",
    "\n",
    "name_dict = {'province':'prov_th', 'des_en':'lucode'}\n",
    "#gdf0 = gdf0.rename(columns= name_dict)\n",
    "gdf1 = gdf1.rename(columns= name_dict)\n",
    "gdf2 = gdf2.rename(columns= name_dict)\n",
    "gdf3 = gdf3.rename(columns= name_dict)\n",
    "\n",
    "#gdf0['province'] = proc_prov(gdf0['province'] )\n",
    "gdf1['lucode'] = proc_label_l3(gdf1['lucode'] )\n",
    "gdf2['lucode'] = proc_label_l3(gdf2['lucode'] )\n",
    "gdf3['lucode'] = proc_label_l3(gdf3['lucode'] )\n",
    "\n",
    "print( gdf1['prov_th'].nunique())\n",
    "print( gdf2['prov_th'].nunique())\n",
    "print( gdf3['prov_th'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = gdf2[~gdf2['prov_th'].isin(gdf1['prov_th'].unique())]\n",
    "print(temp['prov_th'].unique())\n",
    "gdf1 = pd.concat([gdf1, temp], ignore_index=True)\n",
    "\n",
    "temp = gdf3[~gdf3['prov_th'].isin(gdf1['prov_th'].unique())]\n",
    "print(temp['prov_th'].unique())\n",
    "gdf1 = pd.concat([gdf1, temp], ignore_index=True)\n",
    "\n",
    "print(gdf1['prov_th'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional files \n",
    "gdf3 = gpd.read_file(files[i-3]).drop(['start_year', 'end_year'], axis=1)\n",
    "gdf3['province'] = proc_prov_l3(gdf3['province'] )\n",
    "gdf3 = gdf3.rename(columns= name_dict)\n",
    "gdf3['lucode'] = proc_label_l3(gdf3['lucode'] )\n",
    "\n",
    "temp = gdf3[~gdf3['prov_th'].isin(gdf1['prov_th'].unique())]\n",
    "print(temp['prov_th'].unique())\n",
    "gdf1 = pd.concat([gdf1, temp], ignore_index=True)\n",
    "\n",
    "print(gdf1['prov_th'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional files \n",
    "gdf3 = gpd.read_file(files[i-4]).drop(['start_year', 'end_year'], axis=1)\n",
    "gdf3['province'] = proc_prov_l3(gdf3['province'] )\n",
    "gdf3 = gdf3.rename(columns= name_dict)\n",
    "gdf3['lucode'] = proc_label_l3(gdf3['lucode'] )\n",
    "\n",
    "temp = gdf3[~gdf3['prov_th'].isin(gdf1['prov_th'].unique())]\n",
    "print(temp['prov_th'].unique())\n",
    "gdf1 = pd.concat([gdf1, temp], ignore_index=True)\n",
    "\n",
    "print(gdf1['prov_th'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional files \n",
    "gdf3 = gpd.read_file(files[i-5]).drop(['start_year', 'end_year'], axis=1)\n",
    "gdf3['province'] = proc_prov_l3(gdf3['province'] )\n",
    "gdf3 = gdf3.rename(columns= name_dict)\n",
    "gdf3['lucode'] = proc_label_l3(gdf3['lucode'] )\n",
    "\n",
    "temp = gdf3[~gdf3['prov_th'].isin(gdf1['prov_th'].unique())]\n",
    "print(temp['prov_th'].unique())\n",
    "gdf1 = pd.concat([gdf1, temp], ignore_index=True)\n",
    "\n",
    "print(gdf1['prov_th'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = gdf1['prov_th'].unique().tolist()\n",
    "arr1.sort()\n",
    "print(arr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2 = prov_dict['prov_th'].values.tolist()\n",
    "arr2.sort()\n",
    "print(arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([arr1,arr2]).transpose()\n",
    "df[df[0] != df[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_gdf1 = gdf1.merge(prov_dict, on='prov_th', how='left')\n",
    "print('missing prov', prov_dict[~prov_dict['prov_th'].isin(new_gdf1['prov_th'])])\n",
    "new_gdf1[new_gdf1['province'].isna()]['prov_th'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#year = '2550'\n",
    "#year = str(int(year) - 543 )\n",
    "\n",
    "year = new_gdf1['year'].unique()[0]\n",
    "\n",
    "save_folder = proc3_folder + str(year) + '/'\n",
    "try:\n",
    "    os.mkdir(save_folder)\n",
    "except:\n",
    "    pass\n",
    "save_filename = save_folder + str(year) + '.shp'\n",
    "print(save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gdf1['year'] = year\n",
    "\n",
    "new_gdf1[['lucode', 'year', 'geometry', 'province']].to_file(save_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate songkla as another folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2012\n",
    "\n",
    "save_folder = proc3_folder + str(year) + '/'\n",
    "save_filename = save_folder + str(year) + '.shp'\n",
    "print(save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gdf1 = gpd.read_file(save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = gdf1['province'].unique()\n",
    "arr.sort()\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songkhla = gdf1[gdf1['province'] == 'Songkhla']\n",
    "songkhla.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2012\n",
    "\n",
    "save_folder = proc3_folder + str(year) + '_songkhla/'\n",
    "os.mkdir(save_folder)\n",
    "save_filename = save_folder + str(year) + '_songkhla.shp'\n",
    "print(save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songkhla.to_file(save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del gdf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate landuse into provinces to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc3_folder = '../data/landuse_l3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year with landlabel \n",
    "year_range = np.arange(2015, 2021)\n",
    "print(year_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2007\n",
    "\n",
    "save_folder = proc3_folder + str(year) + '/'\n",
    "land_filename = save_folder + str(year) + '.shp'\n",
    "print(land_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new save folder \n",
    "new_save_folder = proc3_folder + str(year) + '_prov/'\n",
    "if not os.path.exists(new_save_folder):\n",
    "    os.mkdir(new_save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_songkhla_year = 2013\n",
    "\n",
    "for year in tqdm(year_range):\n",
    "    \n",
    "    save_folder = proc3_folder + str(year) + '/'\n",
    "    land_filename = save_folder + str(year) + '.shp'\n",
    "    print(land_filename)\n",
    "    \n",
    "    landuse = gpd.read_file(land_filename)\n",
    "    \n",
    "    new_save_folder = proc3_folder + str(year) + '_prov/'\n",
    "    if not os.path.exists(new_save_folder):\n",
    "        os.mkdir(new_save_folder)\n",
    "    print(new_save_folder)\n",
    "    \n",
    "    for prov in landuse['province'].unique():\n",
    "    \n",
    "        sub_landuse = landuse[landuse['province'] == prov]\n",
    "        new_save_filename = new_save_folder + str(year) + '_' + prov + '.shp'\n",
    "        sub_landuse.to_file(new_save_filename)\n",
    "        del sub_landuse\n",
    "    \n",
    "        \n",
    "    if 'Songkhla' in landuse['province'].unique():\n",
    "        last_songkhla_year = year\n",
    "    else:\n",
    "        print('Songkhla not in ', year, \"load \", last_songkhla_year)\n",
    "        save_folder = proc3_folder + str(last_songkhla_year) + '/'\n",
    "        songkhla_filename = save_folder + str(last_songkhla_year) + '.shp'\n",
    "        songkhla =  gpd.read_file(songkhla_filename)\n",
    "        songkhla = songkhla[songkhla['province'] == 'Songkhla']\n",
    "        \n",
    "        new_save_filename = new_save_folder + str(year) + '_' + 'Songkhla' + '.shp'\n",
    "        songkhla.to_file(new_save_filename)\n",
    "        del songkhla\n",
    "        \n",
    "    del landuse \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate Fire data into year, select country and Add provinces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_folder = '../data/world_maps/'\n",
    "mfire_folder = '../data/fire_map/world_2000-2020/M6_proc/'\n",
    "vfire_folder = '../data/fire_map/world_2000-2020/V1_proc/'\n",
    "poll_folder = '../data/poll_map/'\n",
    "thfire_folder = poll_folder + 'th_fire_years/'\n",
    "report_folder = '../reports/map/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process raw fire data. call this function after loading new data from NASA\n",
    "#instr = 'MODIS'\n",
    "instr = 'VIIRS'\n",
    "add_merc_to_fire(instr=instr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_1fire_by_year(file, start_stop_dates, save_prefix, save_folder, timezone='Asia/Bangkok', chunk=1E6):\n",
    "    save_filenames = []\n",
    "    # load fire data  in chunk and append the files to proper year \n",
    "    for fire_df in pd.read_csv(file, chunksize=chunk):\n",
    "        fire_df = process_fire_data(filename=None, fire=fire_df, and_save=False, timezone=timezone, to_drop=False)\n",
    "        for year, start_date, end_date in start_stop_dates:\n",
    "            save_filename = save_folder + 'th_fire_' + save_prefix + str(year) + '.csv'\n",
    "            sub_fire = fire_df.loc[start_date:end_date]\n",
    "            if len(sub_fire) > 0:\n",
    "                # save fire by year \n",
    "                if os.path.exists(save_filename):\n",
    "                    # fire already exist \n",
    "                    exist_sub_fire = pd.read_csv(save_filename)\n",
    "                    exist_sub_fire['datetime'] = pd.to_datetime(exist_sub_fire['datetime'] )\n",
    "                    exist_sub_fire = exist_sub_fire.set_index('datetime')\n",
    "                    sub_fire = pd.concat([sub_fire, exist_sub_fire])\n",
    "                    sub_fire = sub_fire.drop_duplicates()\n",
    "            \n",
    "                sub_fire.to_csv(save_filename, index=True)\n",
    "                save_filenames.append(save_filename)\n",
    "    return save_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_fires_by_year(year_range, save_folder, instr='MODIS', start_season = '07-01', end_season = '06-30', timezone='Asia/Bangkok'):\n",
    "    \n",
    "    # build date start and stop pair \n",
    "    start_list = [f'{y}-{start_season}' for y in year_range]\n",
    "    stop_list = [f'{y+1}-{end_season}' for y in year_range]\n",
    "    start_stop_dates = [*zip(year_range, start_list, stop_list)]\n",
    "    print(start_stop_dates)\n",
    "    \n",
    "    # load all modis fires and save them in proper file by year \n",
    "    if instr == 'MODIS':\n",
    "        raw_folder = '../data/fire_map/world_2000-2020/M6_proc/'\n",
    "        save_prefix = 'm_'\n",
    "        save_folder = save_folder.replace('s/', 's_m/')\n",
    "    elif instr == 'VIIRS':\n",
    "        raw_folder = '../data/fire_map/world_2000-2020/V1_proc/'\n",
    "        save_prefix = 'v_'\n",
    "        save_folder = save_folder.replace('s/', 's_v/')\n",
    "    \n",
    "    if os.path.exists(save_folder):\n",
    "        shutil.rmtree(save_folder)\n",
    "    os.mkdir(save_folder)\n",
    "    \n",
    "    save_filenames = []\n",
    "\n",
    "    files = glob(raw_folder + '*.csv')\n",
    "    print('there are ', len(files) , 'files')\n",
    "    for file in tqdm(files):\n",
    "        save_filenames += split_1fire_by_year(file, start_stop_dates, save_prefix, save_folder=save_folder, timezone=timezone)\n",
    "    \n",
    "    return np.unique(save_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instr = 'MODIS'\n",
    "instr = 'VIIRS'\n",
    "\n",
    "# modis year arange \n",
    "year_range = np.arange(2003, datetime.now().year )\n",
    "\n",
    "# viirs year range \n",
    "# modis year arange \n",
    "year_range = np.arange(2012, datetime.now().year )\n",
    "print(year_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_fires_by_year(year_range=year_range, save_folder=thfire_folder, instr=instr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for modis\n",
    "save_filenames = glob(thfire_folder.replace('s/', 's_m/') + '*.csv')\n",
    "print(len(save_filenames))\n",
    "print(save_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_filenames = glob(thfire_folder.replace('s/', 's_v/') + '*.csv')\n",
    "print(len(save_filenames))\n",
    "print(save_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire = pd.read_csv(save_filenames[0])\n",
    "print(save_filenames[0])\n",
    "print(fire.shape)\n",
    "fire = pd.read_csv(save_filenames[1])\n",
    "print(save_filenames[1])\n",
    "print(fire.shape)\n",
    "fire.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def select_fire_country(save_filenames_list:list, country='Thailand'):\n",
    "    # add country and keep only Thailand, save over the old file \n",
    "    columns_to_keep = ['datetime', 'latitude', 'longitude']\n",
    "    for file in tqdm(save_filenames_list):\n",
    "        fire = pd.read_csv(file)\n",
    "        fire = fire[columns_to_keep]\n",
    "        fire = fire.drop_duplicates(['datetime', 'latitude', 'longitude'])\n",
    "        fire = add_countries(fire)\n",
    "        fire = fire[fire['country'] == country]\n",
    "        fire.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_fire_country(save_filenames_list= save_filenames, country='Thailand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Thailand provincial boundry\n",
    "filename = map_folder + 'THA.gdb'\n",
    "# select province level\n",
    "prov_map = gpd.read_file(filename, driver='FileGDB', layer=2)\n",
    "prov_map['geometry'].shape\n",
    "# overide old crs and convert\n",
    "crs = pyproj.CRS('EPSG:4326')\n",
    "prov_map['geometry'] = prov_map['geometry'].set_crs(crs, allow_override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_province(p, gdf, col='admin1Name_en'):\n",
    "    \"\"\"Find a province hosting the hotspot.\n",
    "\n",
    "    Args:\n",
    "        p: Point object\n",
    "        gdf: geopandas dataframe with albel \n",
    "    \n",
    "    Returns: str \n",
    "        name of the country \n",
    "    \"\"\"\n",
    "    try: \n",
    "        province = gdf[gdf['geometry'].contains(p)][col].values[0]\n",
    "    except: \n",
    "        province = np.nan\n",
    "        \n",
    "    return province"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_provinces(save_filenames_list, prov_map, col='admin1Name_en'):\n",
    "    for file in tqdm(save_filenames_list):\n",
    "        fire = pd.read_csv(file)\n",
    "        # add province \n",
    "        fire['geometry'] = [Point(x,y) for x, y in zip(fire['longitude'], fire['latitude'])]\n",
    "        fire['province'] = fire['geometry'].swifter.apply(locate_province, gdf=prov_map, col=col)\n",
    "        fire = fire.drop('geometry', axis=1)\n",
    "        fire.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_provinces(save_filenames_list=save_filenames, prov_map=prov_map, col='admin1Name_en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(save_filenames[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfire_folder = poll_folder + 'th_fire_years_m/'\n",
    "\n",
    "filenames = glob(mfire_folder + '*.csv')\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check viirs data\n",
    "mfire_folder = poll_folder + 'th_fire_years_v/'\n",
    "\n",
    "filenames = glob(mfire_folder + '*.csv')\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename  in filenames:\n",
    "    fire = pd.read_csv(filename)\n",
    "    if 'long_km' in fire.columns:\n",
    "        fire = fire.drop(['long_km', 'lat_km'], axis=1)\n",
    "        fire.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire = pd.read_csv('../data/poll_map/th_fire_years_v\\\\th_fire_v_2012.csv')\n",
    "fire.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split VIIRS into provinces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check viirs data\n",
    "mfire_folder = poll_folder + 'th_fire_years_v/'\n",
    "\n",
    "filenames = glob(mfire_folder + '*.csv')\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_v_folder = poll_folder + 'th_fire_years_v_prov/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename  in tqdm(filenames):\n",
    "    fire = pd.read_csv(filename)\n",
    "    fire = fire[~fire['province'].isna()]\n",
    "    for prov in fire['province'].unique():\n",
    "        sub_fire = fire[fire['province']==prov]\n",
    "        save_filename = filename.replace('th_fire_years_v', 'th_fire_years_v_prov')\n",
    "        save_filename = save_filename.replace('.csv', f'{prov}.csv')\n",
    "        sub_fire.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label lucode level modis 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (try) merge landuse & Fire data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/48097742/geopandas-point-in-polygon\n",
    "\n",
    "from shapely.geometry import Point, Polygon\n",
    "import geopandas\n",
    "\n",
    "polys = geopandas.GeoSeries({\n",
    "    'foo': Polygon([(5, 5), (5, 13), (13, 13), (13, 5)]),\n",
    "    'bar': Polygon([(10, 10), (10, 15), (15, 15), (15, 10)]),\n",
    "})\n",
    "\n",
    "_pnts = [Point(3, 3), Point(8, 8), Point(11, 11)]\n",
    "pnts = geopandas.GeoDataFrame(geometry=_pnts, index=['A', 'B', 'C'])\n",
    "pnts = pnts.assign(**{key: pnts.within(geom) for key, geom in polys.items()})\n",
    "\n",
    "print(pnts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_folder = '../data/landuse_l3/'\n",
    "label_filename =  label_folder + 'label_all.csv' \n",
    "poll_folder = '../data/poll_map/'\n",
    "# modis\n",
    "mfire_folder = poll_folder + 'th_fire_years_m/'\n",
    "save_folder = poll_folder + 'th_fire_years_m_proc/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year with landlabel \n",
    "year_range = np.arange(2007, 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "year = year_range[0]\n",
    "# load fire data \n",
    "filename = mfire_folder + f'th_fire_m_{year}.csv'\n",
    "save_filename = save_folder + f'th_fire_m_{year}.csv'\n",
    "print('load ' + filename)\n",
    "fire = pd.read_csv(filename)\n",
    "fire = add_merc_col(fire, lat_col='latitude', long_col='longitude', unit='m')\n",
    "land_filename = label_folder + str(year) + '/' + str(year) + '.shp'\n",
    "print('load ' + land_filename)\n",
    "landuse = gpd.read_file(land_filename)\n",
    "\n",
    "use_backup_songkla = 'Songkhla' not in landuse['province'].unique()\n",
    "\n",
    "if (use_backup_songkla):\n",
    "    print('load Songkhla landuse')\n",
    "    songkhla_filename = '../data/landuse_l3/2012_songkhla/2012_songkhla.shp'\n",
    "    songkhla_landuse = gpd.read_file(songkhla_filename)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fire_gdf(fire, buffer=500):\n",
    "    fire_gdf = gpd.GeoDataFrame(fire, geometry=gpd.points_from_xy(fire.longitude, fire.latitude))\n",
    "    fire_gdf = fire_gdf.set_crs(\"EPSG:4326\")\n",
    "    fire_gdf = fire_gdf.to_crs(\"EPSG:3857\")\n",
    "    fire_gdf['geometry'] = fire_gdf['geometry'].buffer(buffer)\n",
    "    return fire_gdf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lucode(idxs, sub_fire, sub_landuse ):\n",
    "    return sjoin(sub_fire.loc[idxs][['geometry']], sub_landuse, how='left')[['lucode']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_province(prov_name, fire_gdf, landuse, chunk=50):\n",
    "    # select data by province \n",
    "    sub_fire = fire_gdf[fire_gdf['province'] == prov_name]\n",
    "    if len(sub_fire) > 0:\n",
    "        sub_landuse = landuse[landuse['province'] == prov_name][['geometry', 'lucode']]\n",
    "\n",
    "        # calculate number of splits\n",
    "        n_splits = ceil(len(sub_fire)/chunk)\n",
    "        idx_splits = np.array_split(sub_fire.index, n_splits)\n",
    "\n",
    "        labeled_all = Parallel(n_jobs=-2)(delayed(extract_lucode)(idxs, sub_fire, sub_landuse) for idxs in idx_splits)\n",
    "        return pd.concat(labeled_all, ignore_index=False)\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "year_label = []\n",
    "for prov_name in tqdm(fire['province'].dropna().unique()):\n",
    "    if (prov_name == 'Songkhla') & (use_backup_songkla):\n",
    "        prov_label = label_province(prov_name, fire_gdf, songkhla_landuse, chunk=50)\n",
    "    else:\n",
    "        prov_label = label_province(prov_name, fire_gdf, landuse, chunk=50)\n",
    "         \n",
    "    year_label.append(prov_label)\n",
    "    \n",
    "year_label = pd.concat(year_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fire = fire.merge(year_label, left_index=True, right_index=True, how='outer')\n",
    "new_fire = new_fire.drop('geometry', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov_label = label_province('Songkhla', fire_gdf, songkhla_landuse, chunk=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fire.to_csv(save_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Label Level 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2_file = '../data/landuse_l2/level2_labels.csv'\n",
    "label2 = pd.read_csv(label2_file)\n",
    "label2 = label2.rename(columns={'lu_des_th': 'des_th_l2', \n",
    "                                'lu_des_en': 'des_en_l2',\n",
    "                                 'lu_code': 'lucode_l2'})\n",
    "label2 = label2.sort_values('lucode_l2')\n",
    "label2 = label2.drop_duplicates('lucode_l2')\n",
    "\n",
    "label3_file = '../data/landuse_l3/level3_labels.csv'\n",
    "label3 = pd.read_csv(label3_file)\n",
    "label3 = label3.rename(columns = {'lucode':'lucode_l3', \n",
    "                        'des_th': 'des_th_l3', \n",
    "                        'des_en':'des_en_l3'})\n",
    "\n",
    "label1  = pd.DataFrame({'A': 'farm and corp',\n",
    "             'F': 'forest',\n",
    "             'M': 'miscellaneous land',\n",
    "             'U': 'built-up land',\n",
    "             'W': 'water'}, index=[0])\n",
    "label1 = label1.transpose().reset_index()\n",
    "label1.columns = ['lucode_l1', 'des_en_l1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not use\n",
    "label1 = label2.copy()\n",
    "label1 = label1.rename(columns={'des_th_l2': 'des_th_l1',\n",
    "                               'des_en_l2': 'des_en_l1',\n",
    "                               'lucode_l2': 'lucode_l1' })\n",
    "label1['lucode_l1'] = label1['lucode_l1'].str[:1]\n",
    "label1 = label1.drop('des_th_l1', axis=1)\n",
    "\n",
    "label1_dict = defaultdict(list)\n",
    "for i, row in label1.iterrows():\n",
    "    label1_dict[row['lucode_l1']].append(row['des_en_l1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_fire_folder = '../data/poll_map/th_fire_years_m_proc/'\n",
    "new_fire_folder = '../data/poll_map/th_fire_years_m_proc_label/'\n",
    "if not os.path.exists(new_fire_folder):\n",
    "    os.mkdir(new_fire_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_files = glob(pro_fire_folder + '*.csv')\n",
    "len(fire_files)\n",
    "print( fire_files[0])\n",
    "print( fire_files[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = fire_files[0]\n",
    "save_filename = filename.replace('th_fire_years_m_proc', 'th_fire_years_m_proc_label')\n",
    "fire = pd.read_csv(filename, encoding='iso_8859_11')\n",
    "if 'geometry' in fire.columns:\n",
    "    fire = fire.drop('geometry', axis=1)\n",
    "\n",
    "fire['lucode'] = fire['lucode'].replace('+', '/')\n",
    "print(fire.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expland_lucode(fire_df):\n",
    "    fire_df['lucode'] = fire_df['lucode'].replace('+', '/')\n",
    "    \n",
    "    clean_fire = fire_df[~fire_df['lucode'].str.contains('/').fillna(False)]\n",
    "    # separate the file with '/'\n",
    "    dirty_fire = fire_df[fire_df['lucode'].str.contains('/').fillna(False)]\n",
    "    \n",
    "    dirty_melt = dirty_fire['lucode'].str.split('/', expand=True).melt( ignore_index=False, value_name='lucode')\n",
    "    dirty_melt = dirty_melt.dropna()\n",
    "    dirty_melt = dirty_melt.drop('variable', axis=1)\n",
    "    \n",
    "    dirty_fire = dirty_fire.drop('lucode',axis=1)\n",
    "    new_dirty_fire = dirty_fire.merge(dirty_melt, left_index=True, right_index=True, how='outer')\n",
    "    \n",
    "    return pd.concat([clean_fire,  new_dirty_fire], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in fire_files:\n",
    "    save_filename = filename.replace('th_fire_years_m_proc', 'th_fire_years_m_proc_label')\n",
    "    fire = pd.read_csv(filename, encoding='iso_8859_11')\n",
    "    if 'geometry' in fire.columns:\n",
    "        fire = fire.drop('geometry', axis=1)\n",
    "\n",
    "    fire['lucode'] = fire['lucode'].replace('+', '/')\n",
    "    # fix lucode with /\n",
    "    fire = expland_lucode(fire)\n",
    "    # merge lucode level 3 \n",
    "    fire = fire.merge(label3, left_on='lucode', right_on='lucode_l3', how='left')\n",
    "    fire['lucode_l2'] = fire['lucode'].str[:2]\n",
    "    # merge lucode level 2 \n",
    "    fire = fire.merge(label2,  on='lucode_l2',  how='left')\n",
    "    #fire = fire.drop(['lucode', 'long_m', 'lat_m'], axis=1)\n",
    "    fire.to_csv(save_filename, index=False, encoding= 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country Level analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def proc_l3_name(df):\n",
    "    des_en_l3_dict = {'active paddy field': 'rice (active paddy field, upland, shifiting cultivation)', \n",
    "                  #'corn(shifting cultivation)': 'corn (normal and shifting cultivation)',\n",
    "                  'upland rice(shifting cultivation)': 'rice (active paddy field, upland, shifiting cultivation)',\n",
    "                  'upland rice': 'rice (active paddy field, upland, shifiting cultivation)' }\n",
    "     \n",
    "        \n",
    "    des_th_l3_dict = {'นาข้าว': 'ข้าว (นาข้าว, หมุนเวียน, ข้าวไร่)', \n",
    "                 # 'ข้าวโพด(ไร่หมุนเวียน)': 'ข้าวโพด',\n",
    "                  'ข้าวไร่(ไร่หมุนเวียน)': 'ข้าว (นาข้าว, หมุนเวียน, ข้าวไร่)',\n",
    "                  'ข้าวไร่': 'ข้าว (นาข้าว, ไร่หมุนเวียน, ข้าวไร่)',\n",
    "                  'กระเจี๊ยบแดง(ไร่หมุนเวียน)': 'กระเจี๊ยบ',\n",
    "                  'กัญชา': 'กัญชา กัญชง',\n",
    "                 'ไผ่(ไผ่ตง ไผ่หวาน ปลูกเพื่อการค้า)': 'ไผ่',\n",
    "                 'ไผ่ (ไผ่หนาม)': 'ไผ่',\n",
    "                 'ป่าดิบรอสภาพฟื้นฟู' : 'ป่าดิบ', \n",
    "                 'ป่าไม่ผลัดใบสมบูรณ์' : 'ป่าดิบ' }\n",
    "    \n",
    "    df['des_en_l3'] = df['des_en_l3'].replace(des_en_l3_dict)\n",
    "    df['des_th_l3'] = df['des_th_l3'].replace(des_th_l3_dict)\n",
    "    \n",
    "    df['des_en_l3'] = df['des_en_l3'].str.replace('\\(shifting cultivation\\)', '')\n",
    "    df['des_en_l3'] = df['des_en_l3'].str.replace('disturbed ', '')\n",
    "    df['des_en_l3'] = df['des_en_l3'].str.replace('dense ', '')\n",
    "    #df['des_en_l3'] = df['des_en_l3'].str.replace('()', '')\n",
    "    \n",
    "    df['des_th_l3'] = df['des_th_l3'].str.replace('\\(ไร่หมุนเวียน\\)', '')\n",
    "    df['des_th_l3'] = df['des_th_l3'].str.replace('รอสภาพฟื้นฟู', '')\n",
    "    df['des_th_l3'] = df['des_th_l3'].str.replace('สมบูรณ์', '')\n",
    "    #df['des_th_l3'] = df['des_th_l3'].str.replace('()', '')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_folder = 'C:/Users/Benny/Documents/Fern/aqi_thailand2/reports/Thailand/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_range = np.arange(2007, 2020)\n",
    "year_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_filenames = [f'../data/poll_map/th_fire_years_m_proc_label/th_fire_m_{year}.csv' for year in year_range]\n",
    "fire_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "fire_year_all = []\n",
    "code_df = []\n",
    "for year, file in zip(year_range, fire_filenames):\n",
    "    fire = pd.read_csv(file)\n",
    "    fire = proc_l3_name(fire)\n",
    "    code_df.append(fire[['lucode_l3', 'des_en_l3']].drop_duplicates())\n",
    "    fire_year = fire.groupby(['lucode_l3', 'des_en_l3', 'des_th_l3']).count()[['latitude']]\n",
    "    fire_year.columns = [year]\n",
    "    fire_year_all.append(fire_year)\n",
    "    \n",
    "fire_year_all = pd.concat(fire_year_all, axis=1)\n",
    "fire_year_all = fire_year_all.fillna(0)\n",
    "fire_year_all = fire_year_all.astype(int)\n",
    "fire_year_all = fire_year_all.sort_values(year_range[-1], ascending=False)\n",
    "\n",
    "code_df = pd.concat(code_df)\n",
    "code_df = code_df.dropna()\n",
    "code_df = code_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_year_all.to_csv('../data/poll_map/level3_year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_list = ['river, canal', 'river, canal', 'farm pond' ]\n",
    "fire_to_plot = fire_year_all.head(10 + len(ignore_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_to_plot = fire_to_plot.reset_index()\n",
    "fire_to_plot = fire_to_plot.drop(['lucode_l3', 'des_th_l3'], axis = 1)\n",
    "fire_to_plot = fire_to_plot[~fire_to_plot['des_en_l3'].isin(ignore_list)]\n",
    "fire_to_plot = fire_to_plot.set_index('des_en_l3') \n",
    "temp = fire_to_plot.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.set_title('top 10 hotspot landuse (Thailand)')\n",
    "temp.plot(marker='x', ax=ax)\n",
    "labels = [ '\\n'.join(wrap(l, 20)) for l in temp.columns]\n",
    "\n",
    "ax.legend( labels, bbox_to_anchor=(1.05, 1), fontsize=12)\n",
    "ax.set_ylabel('number of spots(MODIS)')\n",
    "ax.set_xlabel('year')\n",
    "plt.tight_layout()\n",
    "plt.savefig(report_folder + 'country_top_10_hotspots.png', dpi=300 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pie chart "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_fire_mean(fire_year):\n",
    "    \n",
    "    fire_mean = fire_year.median(axis=1).round(0) \n",
    "    \n",
    "    fire_mean.name = 'mean_spot'\n",
    "    fire_mean = fire_mean.reset_index()\n",
    "    fire_mean = fire_mean[['lucode_l3', 'des_en_l3', 'mean_spot']]\n",
    "    fire_mean['lucode_l2'] = fire_mean['lucode_l3'].str[:2]\n",
    "    fire_mean['lucode_l1'] = fire_mean['lucode_l2'].str[:1]\n",
    "    fire_mean = fire_mean.merge(label2, on ='lucode_l2', how='left')\n",
    "    fire_mean = fire_mean.merge(label1, on ='lucode_l1', how='left')\n",
    "    return fire_mean\n",
    "\n",
    "def add_degree(df, round_level =3):\n",
    "    # convert to percent, add degree for pie chart\n",
    "    df['spot_per'] = df['mean_spot']/df['mean_spot'].sum() \n",
    "    df['spot_per'] = df['spot_per'].astype(float).round(round_level)\n",
    "    df['degree'] = df['spot_per']*360 \n",
    "    df['degree'] = df['degree'].astype(int)\n",
    "    return df\n",
    "    \n",
    "def cal_l1_group(fire_mean):\n",
    "    # level 1 group\n",
    "    l1_count = fire_mean.groupby(['des_en_l1'], as_index=False).sum()\n",
    "    l1_count = l1_count.sort_values('des_en_l1', ascending=False)\n",
    "    l1_count = add_degree(l1_count)\n",
    "    \n",
    "    return l1_count \n",
    "\n",
    "def count_sub_fire(mean_data, col = 'des_en_l3', header_num = 5):\n",
    "    # level 3 group for crops\n",
    "    # \"for subgroup of either level 2, level 3\"\n",
    "    l_count = mean_data.groupby([col], as_index=False).sum()\n",
    "    l_count = l_count.sort_values('mean_spot', ascending=False)\n",
    "\n",
    "    to_keep = l_count.head(header_num) \n",
    "\n",
    "    other_count = l_count[~l_count[col].isin(to_keep[col])]\n",
    "    other_count = pd.DataFrame(other_count.sum()).transpose()\n",
    "    other_count[col] = 'others'\n",
    "\n",
    "    l_count = pd.concat([to_keep, other_count])\n",
    "    l_count = l_count.sort_values(col, ascending=True)\n",
    "    l_count[col] = l_count[col].str.replace('\\(active paddy field, upland, shifiting cultivation\\)', '')\n",
    "    # convert to percent \n",
    "    l_count = add_degree(l_count)\n",
    "    return l_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pie(df, col, title='', explode = [], filename=None, startangle=45):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(11,8))\n",
    "    \n",
    "    if len(explode) == 0:\n",
    "        explode = tuple(np.zeros(len(df)))\n",
    "    \n",
    "    wedges, texts, autotexts = ax.pie(df['degree'], autopct='%1.1f%%', startangle=startangle, labels = df[col].to_list(), explode= explode, shadow=False )\n",
    "    \n",
    "    #wedges, texts, autotexts = ax.pie(df['degree'], autopct='%1.1f%%', startangle=startangle, labels = df[col].to_list(), shadow=False )\n",
    "     \n",
    "    ax.axis('equal')\n",
    "    for text in texts:\n",
    "        text.set_fontsize(20)  \n",
    "   \n",
    "    labels = [ str(round(p*100/360, 1)) +'%' + ' '+s   for s, p in zip(df[col], df['degree'])]\n",
    "\n",
    "    #ax.legend(wedges, labels,\n",
    "    #          title=\"factors\",\n",
    "    #          loc=\"upper right\", bbox_to_anchor=(1.3, 0.9))\n",
    "    plt.title(title)\n",
    "    plt.setp(autotexts, size=20, weight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    if filename:\n",
    "        plt.savefig(filename, dpi=300)\n",
    "        \n",
    "        \n",
    "def plot_bar(df, x, y, title='', filename=None):\n",
    "    _, ax = plt.subplots(figsize=(10, 6))\n",
    "    df.plot(x, y, kind='bar' , color=[ u'#1f77b4', u'#ff7f0e', u'#2ca02c', u'#d62728' ], ax=ax,\n",
    "            linewidth=1,\n",
    "            edgecolor='black',\n",
    "            legend=False, error_kw=dict(ecolor='black', lw=1, capsize=4, capthick=1))\n",
    "\n",
    "    ax.set_ylabel(y)\n",
    "    ax.set_xlabel(None)\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if filename:\n",
    "        plt.savefig(filename, dpi=300)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_mean = cal_fire_mean(fire_year_all)\n",
    "print(fire_mean.shape)\n",
    "fire_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level 1 group\n",
    "l1_count = cal_l1_group(fire_mean)\n",
    "\n",
    "l1_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pie(l1_count, col='des_en_l1', title='Country Level Landuse Hotspot', filename=report_folder + 'country_l1_pie.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fire_mean[fire_mean['des_en_l1'] == 'farm and corp']\n",
    "l_count = count_sub_fire(df, col = 'des_en_l3', header_num = 6)\n",
    "l_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_count.loc[93, 'des_en_l3'] = 'rice'\n",
    "plot_pie(l_count, col='des_en_l3', title='Farm and Crop Hotspots (Country Level)', filename=report_folder + 'country_farm_pie.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_bar(l_count, 'des_en_l3', 'mean_spot', title='Farm and Crop Hotspots (Country Level)')\n",
    "ax.set_xlabel('crop type')\n",
    "ax.set_ylabel('median number hotspots\\n (2007- 2019)')\n",
    "plt.savefig(report_folder + 'country_farm_bar.png', dpi=300 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChiangMai Hotspot analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('Chiang Mai')\n",
    "y = dataset.city_info['long_m']\n",
    "x = dataset.city_info['lat_m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_folder = 'C:/Users/Benny/Documents/Fern/aqi_thailand2/reports/Thailand/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_range = np.arange(2007, 2020)\n",
    "print(year_range)\n",
    "fire_filenames = [f'../data/poll_map/th_fire_years_m_proc_label/th_fire_m_{year}.csv' for year in year_range]\n",
    "print(fire_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = 200E3\n",
    "fire_year_all = []\n",
    "\n",
    "for year, file in zip(year_range, fire_filenames):\n",
    "    fire = pd.read_csv(file)\n",
    "    # keep fire close to the city \n",
    "    fire['distance'] = np.sqrt((fire['lat_m'] - x)**2 + (fire['long_m'] - y)**2)\n",
    "    fire = fire[fire['distance'] <= distance]\n",
    "    \n",
    "    fire = proc_l3_name(fire)\n",
    "    fire_year = fire.groupby(['lucode_l3', 'des_en_l3', 'des_th_l3']).count()[['latitude']]\n",
    "    fire_year.columns = [year]\n",
    "    fire_year_all.append(fire_year)\n",
    "    \n",
    "fire_year_all = pd.concat(fire_year_all, axis=1)\n",
    "fire_year_all = fire_year_all.fillna(0)\n",
    "fire_year_all = fire_year_all.astype(int)\n",
    "fire_year_all = fire_year_all.sort_values(year_range[-1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_year_all.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_list = ['river, canal', 'river, canal', 'farm pond', 'thai village', 'hill tribe village']\n",
    "fire_to_plot = fire_year_all.head(10 + len(ignore_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_to_plot = fire_to_plot.reset_index()\n",
    "fire_to_plot = fire_to_plot.drop(['des_th_l3', 'lucode_l3'], axis = 1)\n",
    "fire_to_plot = fire_to_plot[~fire_to_plot['des_en_l3'].isin(ignore_list)]\n",
    "fire_to_plot = fire_to_plot.set_index('des_en_l3') \n",
    "temp = fire_to_plot.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.set_title(f'top 10 hotspot landuse {dataset.city_name}')\n",
    "temp.plot(marker='x', ax=ax)\n",
    "labels = [ '\\n'.join(wrap(l, 20)) for l in temp.columns]\n",
    "\n",
    "ax.legend( labels, bbox_to_anchor=(1.05, 1), fontsize=11)\n",
    "ax.set_ylabel('number of spots(MODIS)')\n",
    "ax.set_xlabel('year')\n",
    "plt.tight_layout()\n",
    "plt.savefig(report_folder + f'{dataset.city_name}_{int(distance/1000)}_top_10_hotspots.png', dpi=300 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_mean = cal_fire_mean(fire_year_all)\n",
    "print(fire_mean.shape)\n",
    "fire_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level 1 group\n",
    "l1_count = cal_l1_group(fire_mean)\n",
    "\n",
    "l1_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pie(l1_count, col='des_en_l1', title='Chiang Mai 200km Landuse Hotspot', filename=report_folder + f'{dataset.city_name}_{int(distance/1000)}_l1_pie.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fire_mean[fire_mean['des_en_l1'] == 'farm and corp']\n",
    "l_count = count_sub_fire(df, col = 'des_en_l3', header_num = 6)\n",
    "l_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_count.loc[75, 'des_en_l3'] = 'rice'\n",
    "plot_pie(l_count, col='des_en_l3', title='Farm and Crop Hotspots (Chiang Mai (200km) Level)', startangle=0, explode=(0,0,0,0,0.1,0.2,0), filename=report_folder + f'{dataset.city_name}_{int(distance/1000)}_farm_pie.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_bar(l_count, 'des_en_l3', 'mean_spot', title='Farm and Crop Hotspots (Chiang Mai (200 km))')\n",
    "ax.set_xlabel('crop type')\n",
    "ax.set_ylabel('median number hotspots\\n (2007- 2019)')\n",
    "plt.savefig(report_folder + f'{dataset.city_name}_{int(distance/1000)}_farm_bar.png', dpi=300 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bangkok Hotspot analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('Bangkok')\n",
    "y = dataset.city_info['long_m']\n",
    "x = dataset.city_info['lat_m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_folder = 'C:/Users/Benny/Documents/Fern/aqi_thailand2/reports/Thailand/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_range = np.arange(2007, 2020)\n",
    "print(year_range)\n",
    "fire_filenames = [f'../data/poll_map/th_fire_years_m_proc_label/th_fire_m_{year}.csv' for year in year_range]\n",
    "print(fire_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = 200E3\n",
    "fire_year_all = []\n",
    "\n",
    "for year, file in zip(year_range, fire_filenames):\n",
    "    fire = pd.read_csv(file)\n",
    "    # keep fire close to the city \n",
    "    fire['distance'] = np.sqrt((fire['lat_m'] - x)**2 + (fire['long_m'] - y)**2)\n",
    "    fire = fire[fire['distance'] <= distance]\n",
    "    \n",
    "    fire = proc_l3_name(fire)\n",
    "    fire_year = fire.groupby(['lucode_l3', 'des_en_l3', 'des_th_l3']).count()[['latitude']]\n",
    "    fire_year.columns = [year]\n",
    "    fire_year_all.append(fire_year)\n",
    "    \n",
    "fire_year_all = pd.concat(fire_year_all, axis=1)\n",
    "fire_year_all = fire_year_all.fillna(0)\n",
    "fire_year_all = fire_year_all.astype(int)\n",
    "fire_year_all = fire_year_all.sort_values(year_range[-1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_year_all.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_list = ['river, canal', 'river, canal', 'farm pond', 'thai village', 'hill tribe village', 'road', 'farm pond','irrigation canal']\n",
    "fire_to_plot = fire_year_all.head(10 + len(ignore_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_to_plot = fire_to_plot.reset_index()\n",
    "fire_to_plot = fire_to_plot.drop(['des_th_l3', 'lucode_l3'], axis = 1)\n",
    "fire_to_plot = fire_to_plot[~fire_to_plot['des_en_l3'].isin(ignore_list)]\n",
    "fire_to_plot = fire_to_plot.set_index('des_en_l3') \n",
    "temp = fire_to_plot.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.set_title(f'top 10 hotspot landuse {dataset.city_name}')\n",
    "temp.plot(marker='x', ax=ax)\n",
    "labels = [ '\\n'.join(wrap(l, 20)) for l in temp.columns]\n",
    "\n",
    "ax.legend( labels, bbox_to_anchor=(1.05, 1), fontsize=11)\n",
    "ax.set_ylabel('number of spots(MODIS)')\n",
    "ax.set_xlabel('year')\n",
    "plt.tight_layout()\n",
    "plt.savefig(report_folder + f'{dataset.city_name}_{int(distance/1000)}_top_10_hotspots.png', dpi=300 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_mean = cal_fire_mean(fire_year_all)\n",
    "print(fire_mean.shape)\n",
    "fire_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level 1 group\n",
    "l1_count = cal_l1_group(fire_mean)\n",
    "\n",
    "l1_count = l1_count.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pie(l1_count, col='des_en_l1', title=f'{dataset.city_name} 200km Landuse Hotspot', filename=report_folder + f'{dataset.city_name}_{int(distance/1000)}_l1_pie.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fire_mean[fire_mean['des_en_l1'] == 'farm and corp']\n",
    "l_count = count_sub_fire(df, col = 'des_en_l3', header_num = 6)\n",
    "l_count = l_count.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_count.loc[77, 'des_en_l3'] = 'rice'\n",
    "plot_pie(l_count, col='des_en_l3', title=f'Farm and Crop Hotspots ({dataset.city_name} (200km) Level)', startangle=0, filename=report_folder + f'{dataset.city_name}_{int(distance/1000)}_farm_pie.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_bar(l_count, 'des_en_l3', 'mean_spot', title=f'Farm and Crop Hotspots ({dataset.city_name} (200 km))')\n",
    "ax.set_xlabel('crop type')\n",
    "ax.set_ylabel('median number hotspots\\n (2007- 2019)')\n",
    "plt.savefig(report_folder + f'{dataset.city_name}_{int(distance/1000)}_farm_bar.png', dpi=300 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Province with MostHotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = map_folder + 'THA.gdb'\n",
    "# select province level\n",
    "prov_map = gpd.read_file(filename, driver='FileGDB', layer=2)\n",
    "prov_map['geometry'].shape\n",
    "# overide old crs and convert\n",
    "crs = pyproj.CRS('EPSG:4326')\n",
    "prov_map['geometry'] = prov_map['geometry'].set_crs(crs, allow_override=True)\n",
    "prov_map['geometry'] = prov_map['geometry'].to_crs('EPSG:3857')\n",
    "prov_map[\"area(km2)\"] = prov_map['geometry'].area/ 10**6\n",
    "prov_area_df = prov_map[['admin1Name_en', \"area(km2)\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find province wtih most hotspot\n",
    "top_provs = []\n",
    "for year, file in zip(year_range, fire_filenames):\n",
    "    fire = pd.read_csv(file)\n",
    "    fire = proc_l3_name(fire)\n",
    "    fire_year = fire.groupby('province').count()[['lucode_l3']]\n",
    "    fire_year.columns = [year]\n",
    "    top_provs.append(fire_year)\n",
    "    \n",
    "top_provs = pd.concat(top_provs, axis=1)\n",
    "top_provs = top_provs.fillna(0)\n",
    "top_provs = top_provs.astype(int)\n",
    "top_provs = top_provs.sort_values(year_range[-1], ascending=False)\n",
    "top_provs['mean'] = top_provs.mean(axis=1)\n",
    "top_provs = top_provs.reset_index()\n",
    "top_provs = top_provs.merge(prov_area_df, left_on='index', right_on = 'admin1Name_en', how='left')\n",
    "top_provs['mean/area'] = top_provs['mean']/ top_provs[\"area(km2)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov_list = top_provs.sort_values('mean', ascending=False).head()['index'].to_list() + top_provs.sort_values('mean/area', ascending=False).head()['index'].to_list()\n",
    "prov_list = np.unique(prov_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_year_all = []\n",
    "\n",
    "for year, file in zip(year_range, fire_filenames):\n",
    "    fire = pd.read_csv(file)\n",
    "    fire = proc_l3_name(fire)\n",
    "    fire_year = fire.groupby(['province', 'des_en_l3']).count()[['lucode_l3']]\n",
    "    fire_year.columns = [year]\n",
    "    fire_year_all.append(fire_year)\n",
    "    \n",
    "fire_year_all = pd.concat(fire_year_all, axis=1)\n",
    "fire_year_all = fire_year_all.fillna(0)\n",
    "fire_year_all = fire_year_all.astype(int)\n",
    "fire_year_all = fire_year_all.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ignore_list = ['river, canal', 'river, canal', 'farm pond', 'thai village', 'hill tribe village']\n",
    "for prov in prov_list:\n",
    "    prov_fire = fire_year_all[fire_year_all['province'] == prov]\n",
    "    prov_fire = prov_fire.sort_values(year_range[-1], ascending=False)\n",
    "    \n",
    "    fire_to_plot = prov_fire.head(10 + len(ignore_list))\n",
    "    fire_to_plot = fire_to_plot[~fire_to_plot['des_en_l3'].isin(ignore_list)]\n",
    "    fire_to_plot = fire_to_plot.set_index('des_en_l3') \n",
    "    fire_to_plot = fire_to_plot.drop('province', axis=1)\n",
    "    temp = fire_to_plot.transpose()\n",
    "    \n",
    "    _, ax = plt.subplots(figsize=(12, 5))\n",
    "    ax.set_title(f'{prov} top 10 hotspot landuse')\n",
    "    temp.plot(marker='x', ax=ax)\n",
    "    labels = [ '\\n'.join(wrap(l, 20)) for l in temp.columns]\n",
    "\n",
    "    ax.legend( labels, bbox_to_anchor=(1.05, 1), fontsize=11)\n",
    "    ax.set_ylabel('number of spots(MODIS)')\n",
    "    ax.set_xlabel('year')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(report_folder + f'{prov}_top_10_hotspots.png', dpi=300 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regional analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdataset = MapDataset('Thailand')\n",
    "# load station and geopanda file for Thailand\n",
    "mdataset.load_()\n",
    "# load provinces & region information \n",
    "provinces = mdataset.prov_map[['region', 'province']]\n",
    "provinces['region'] = provinces['region'].str.replace('Greater Bangkok', 'Central Region')\n",
    "provinces['area(km2)'] = mdataset.prov_map['geometry'].area/ 10**6\n",
    "region_area_df = provinces.groupby('region', as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find province wtih most hotspot\n",
    "top_provs = []\n",
    "for year, file in zip(year_range, fire_filenames):\n",
    "    fire = pd.read_csv(file)\n",
    "    fire = proc_l3_name(fire)\n",
    "    fire = fire.merge(provinces, on='province', how='left')\n",
    "    fire_year = fire.groupby('region').count()[['lucode_l3']]\n",
    "    fire_year.columns = [year]\n",
    "    top_provs.append(fire_year)\n",
    "    \n",
    "top_provs = pd.concat(top_provs, axis=1)\n",
    "top_provs = top_provs.fillna(0)\n",
    "top_provs = top_provs.astype(int)\n",
    "top_provs = top_provs.sort_values(year_range[-1], ascending=False)\n",
    "top_provs['mean'] = top_provs.mean(axis=1)\n",
    "top_provs = top_provs.reset_index()\n",
    "top_provs = top_provs.merge(region_area_df, on = 'region', how='left')\n",
    "top_provs['mean/area'] = top_provs['mean']/ top_provs[\"area(km2)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_provs.sort_values('mean', ascending=False).head()['region'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_provs.sort_values('mean/area', ascending=False).head()['region'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_year_all = []\n",
    "\n",
    "for year, file in zip(year_range, fire_filenames):\n",
    "    fire = pd.read_csv(file)\n",
    "    fire = proc_l3_name(fire)\n",
    "    fire = fire.merge(provinces, on='province', how='left')\n",
    "    fire_year = fire.groupby(['region', 'lucode_l3', 'des_en_l3']).count()[['latitude']]\n",
    "    fire_year.columns = [year]\n",
    "    fire_year_all.append(fire_year)\n",
    "    \n",
    "fire_year_all = pd.concat(fire_year_all, axis=1)\n",
    "fire_year_all = fire_year_all.fillna(0)\n",
    "fire_year_all = fire_year_all.astype(int)\n",
    "fire_year_all = fire_year_all.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ignore_list = ['river, canal', 'river, canal', 'farm pond', 'thai village', 'hill tribe village', 'road', 'institutional land']\n",
    "\n",
    "for region in provinces['region'].unique():\n",
    "    prov_fire = fire_year_all[fire_year_all['region'] == region]\n",
    "    prov_fire = prov_fire.sort_values(year_range[-1], ascending=False)\n",
    "    \n",
    "    fire_to_plot = prov_fire.head(10 + len(ignore_list))\n",
    "    fire_to_plot = fire_to_plot[~fire_to_plot['des_en_l3'].isin(ignore_list)]\n",
    "    fire_to_plot = fire_to_plot.set_index('des_en_l3') \n",
    "    fire_to_plot = fire_to_plot.drop(['region', 'lucode_l3'], axis=1)\n",
    "    temp = fire_to_plot.transpose()\n",
    "    print(region, \":\", fire_to_plot.head(5).index.to_list())\n",
    "    \n",
    "    _, ax = plt.subplots(figsize=(12, 5))\n",
    "    ax.set_title(f'{region} top 10 hotspot landuse')\n",
    "    temp.plot(marker='x', ax=ax)\n",
    "    labels = [ '\\n'.join(wrap(l, 20)) for l in temp.columns]\n",
    "\n",
    "    ax.legend( labels, bbox_to_anchor=(1.05, 1), fontsize=12)\n",
    "    ax.set_ylabel('number of spots(MODIS)')\n",
    "    ax.set_xlabel('year')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(report_folder + f'{region}_top_10_hotspots.png', dpi=300 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_year_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for region in provinces['region'].unique():\n",
    "    prov_fire = fire_year_all[fire_year_all['region'] == region]\n",
    "    prov_fire = prov_fire.drop('region', axis=1)\n",
    "    prov_fire = prov_fire.set_index(['lucode_l3', 'des_en_l3'])\n",
    "    fire_mean = cal_fire_mean(prov_fire)\n",
    "    # level 1 group\n",
    "    l1_count = cal_l1_group(fire_mean)\n",
    "    plot_pie(l1_count, col='des_en_l1', title=f'{region} Landuse Hotspot', filename=report_folder + f'{region}_l1_pie.png')\n",
    "    \n",
    "    df = fire_mean[fire_mean['des_en_l1'] == 'farm and corp']\n",
    "    l_count = count_sub_fire(df, col = 'des_en_l3', header_num = 6)\n",
    "    l_count['des_en_l3'] = l_count['des_en_l3'].str.replace('\\(active paddy field, upland, shifiting cultivation\\)', '')\n",
    "    plot_pie(l_count, col='des_en_l3', title=f'Farm and Crop Hotspots ({region})', startangle=0, filename=report_folder + f'{region}_farm_pie.png')\n",
    "    \n",
    "    ax = plot_bar(l_count, 'des_en_l3', 'mean_spot', title=f'Farm and Crop Hotspots ({region})')\n",
    "    ax.set_xlabel('crop type')\n",
    "    ax.set_ylabel('median number hotspots\\n (2007- 2019)')\n",
    "    plt.savefig(report_folder + f'{region}_farm_bar.png', dpi=300 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label lucode level VIIRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov_folder = '../data/poll_map/th_fire_years_v_prov_proc/'\n",
    "pro_fire_folder = '../data/poll_map/th_fire_years_v_proc/'\n",
    "new_fire_folder = '../data/poll_map/th_fire_years_v_proc_label/'\n",
    "report_folder = 'C:/Users/Benny/Documents/Fern/aqi_thailand2/reports/Thailand/'\n",
    "if not os.path.exists(new_fire_folder):\n",
    "    os.mkdir(new_fire_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process province files \n",
    "year = 2013\n",
    "prov_files = glob(prov_folder + '*.csv')\n",
    "year_save_filename = pro_fire_folder + f'th_fire_v_{year}.csv'\n",
    "label_filename = new_fire_folder   + f'th_fire_v_{year}.csv'\n",
    "prov_files = [s for s in files if str(year) in s]\n",
    "print('prov_file lenght ', len(prov_files))\n",
    "print('save to file ' + year_safe_filename)\n",
    "print('label to file ' + label_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all province files and save \n",
    "year_fire = []\n",
    "for file in prov_files:\n",
    "\n",
    "    year_fire.append(pd.read_csv(file))\n",
    "\n",
    "year_fire = pd.concat(year_fire)\n",
    "year_fire.to_csv(year_save_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_fire.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labels \n",
    "fire_files = [year_save_filename]\n",
    "for filename in fire_files:\n",
    "    save_filename = filename.replace('th_fire_years_v_proc', 'th_fire_years_v_proc_label')\n",
    "    print(save_filename)\n",
    "    fire = pd.read_csv(filename, encoding='iso_8859_11')\n",
    "    if 'geometry' in fire.columns:\n",
    "        fire = fire.drop('geometry', axis=1)\n",
    "\n",
    "    fire['lucode'] = fire['lucode'].replace('+', '/')\n",
    "    # fix lucode with /\n",
    "    fire = expland_lucode(fire)\n",
    "    # merge lucode level 3 \n",
    "    fire = fire.merge(label3, left_on='lucode', right_on='lucode_l3', how='left')\n",
    "    fire['lucode_l2'] = fire['lucode'].str[:2]\n",
    "    # merge lucode level 2 \n",
    "    fire = fire.merge(label2,  on='lucode_l2',  how='left')\n",
    "    #fire = fire.drop(['lucode', 'long_m', 'lat_m'], axis=1)\n",
    "    fire.to_csv(save_filename, index=False, encoding= 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country Level: Compare MODIS and VIIRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_list = ['river, canal', 'river, canal', 'farm pond', 'thai village', 'hill tribe village', 'road', 'farm pond','irrigation canal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "year = 2013\n",
    "v_fire = pd.read_csv(f'../data/poll_map/th_fire_years_v_proc_label/th_fire_v_{year}.csv')\n",
    "v_fire = proc_l3_name(v_fire)\n",
    "v_fire = v_fire.groupby(['lucode_l3', 'des_en_l3', 'des_th_l3']).count()[['latitude']]\n",
    "v_fire  = cal_fire_mean(v_fire)\n",
    "print(v_fire.shape) \n",
    "v_l1_count = cal_l1_group(v_fire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_fire = pd.read_csv(f'../data/poll_map/th_fire_years_m_proc_label/th_fire_m_{year}.csv') \n",
    "m_fire = proc_l3_name(m_fire)\n",
    "m_fire = m_fire.groupby(['lucode_l3', 'des_en_l3', 'des_th_l3']).count()[['latitude']]\n",
    "m_fire  = cal_fire_mean(m_fire)\n",
    "print(m_fire.shape)\n",
    "# level 1 group\n",
    "m_l1_count = cal_l1_group(m_fire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pie(m_l1_count, col='des_en_l1', title=f'Country Level Landuse Hotspot MODIS {year}', filename=report_folder + f'modis_{year}_country_l1_pie.png', startangle=-45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pie(v_l1_count, col='des_en_l1', title=f'Country Level Landuse Hotspot VIIRS {year}', filename=report_folder + f'viirs_{year}_country_l1_pie.png', startangle=-45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = m_fire[m_fire['des_en_l1'] == 'farm and corp']\n",
    "m_l_count = count_sub_fire(df, col = 'des_en_l3', header_num = 5)\n",
    "m_l_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = v_fire[v_fire['des_en_l1'] == 'farm and corp']\n",
    "v_l_count = count_sub_fire(df, col = 'des_en_l3', header_num = 5)\n",
    "v_l_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pie(m_l_count, col='des_en_l3', title='Farm and Crop Hotspots (Country Level) (MODIS top 5)', filename=report_folder + f'modis_{year}_country_farm_pie.png', startangle=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pie(v_l_count, col='des_en_l3', title='Farm and Crop Hotspots (Country Level) (VIIRS top 5)', filename=report_folder + f'viirs_{year}_country_farm_pie.png', startangle=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bangkok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('Bangkok')\n",
    "y = dataset.city_info['long_m']\n",
    "x = dataset.city_info['lat_m']\n",
    "distance = 200E3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "year = 2013\n",
    "v_fire = pd.read_csv(f'../data/poll_map/th_fire_years_v_proc_label/th_fire_v_{year}.csv')\n",
    "v_fire['distance'] = np.sqrt((v_fire['lat_m'] - x)**2 + (v_fire['long_m'] - y)**2)\n",
    "v_fire = v_fire[v_fire['distance'] <= distance]\n",
    "v_fire = proc_l3_name(v_fire)\n",
    "v_fire = v_fire.groupby(['lucode_l3', 'des_en_l3', 'des_th_l3']).count()[['latitude']]\n",
    "v_fire  = cal_fire_mean(v_fire)\n",
    "print(v_fire.shape) \n",
    "v_l1_count = cal_l1_group(v_fire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_fire = pd.read_csv(f'../data/poll_map/th_fire_years_m_proc_label/th_fire_m_{year}.csv') \n",
    "m_fire['distance'] = np.sqrt((m_fire['lat_m'] - x)**2 + (m_fire['long_m'] - y)**2)\n",
    "m_fire = m_fire[m_fire['distance'] <= distance]\n",
    "m_fire = proc_l3_name(m_fire)\n",
    "m_fire = m_fire.groupby(['lucode_l3', 'des_en_l3', 'des_th_l3']).count()[['latitude']]\n",
    "m_fire  = cal_fire_mean(m_fire)\n",
    "print(m_fire.shape)\n",
    "# level 1 group\n",
    "m_l1_count = cal_l1_group(m_fire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pie(m_l1_count, col='des_en_l1', title=f'{dataset.city_name} 200km Landuse Hotspot(MODIS)', filename=report_folder + f'modis_{year}_{dataset.city_name}_{int(distance/1000)}_l1_pie.png', startangle=-45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pie(m_l1_count, col='des_en_l1', title=f'{dataset.city_name} 200km Landuse Hotspot(VIIRS)', filename=report_folder + f'viirs_{year}_{dataset.city_name}_{int(distance/1000)}_l1_pie.png', startangle=-45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = m_fire[m_fire['des_en_l1'] == 'farm and corp']\n",
    "m_l_count = count_sub_fire(df, col = 'des_en_l3', header_num = 5)\n",
    "m_l_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = v_fire[v_fire['des_en_l1'] == 'farm and corp']\n",
    "v_l_count = count_sub_fire(df, col = 'des_en_l3', header_num = 5)\n",
    "v_l_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pie(m_l_count, col='des_en_l3', title=f'Farm and Crop Hotspots ({dataset.city_name} 200km) (MODIS top 5)', filename=report_folder + f'modis_{year}_{dataset.city_name}_{int(distance/1000)}_farm_pie.png', startangle=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pie(v_l_count, col='des_en_l3', title=f'Farm and Crop Hotspots ({dataset.city_name} 200km) (VIIRS top 5)', filename=report_folder + f'viirs_{year}_{dataset.city_name}_{int(distance/1000)}_farm_pie.png', startangle=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "372.364px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
